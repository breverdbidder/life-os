"""
Article Parser Agent V2.2 - WITH MCP PLAYWRIGHT INTEGRATION
============================================================
SOLVES: JS-rendered sites like PathForge (React SPAs)

STRATEGY:
1A. Jina AI (FREE) - Fast, works for 85% of sites
1B. Playwright MCP (FREE) - Your existing MCP for JS-rendered sites  
2. Trafilatura (FREE) - Simple sites fallback
3. BeautifulSoup (FREE) - Basic fallback
4. Firecrawl (PAID $16/mo) - Last resort

MCP INTEGRATION:
Uses your existing Playwright Chromium MCP from Dec 23 deployment.
Files: mcp_nodes.py, .mcp.json, orchestrator_mcp.yml

Version: 2.2.0
Date: 2025-12-30
"""

import os
import re
import json
import httpx
import hashlib
from datetime import datetime, timezone
from typing import Optional, List, Dict, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import html2text


# Check if MCP is available
MCP_AVAILABLE = os.getenv('USE_MCP', 'true').lower() == 'true'


class ArticleSource(Enum):
    """Source of article extraction"""
    JINA_AI = "jina_ai"
    PLAYWRIGHT_MCP = "playwright_mcp"  # NEW!
    TRAFILATURA = "trafilatura"
    BEAUTIFULSOUP = "beautifulsoup"
    FIRECRAWL = "firecrawl"
    FAILED = "failed"


class LifeOSDomain(Enum):
    """Life OS categorization domains"""
    BUSINESS = "BUSINESS"
    MICHAEL = "MICHAEL"
    FAMILY = "FAMILY"
    PERSONAL = "PERSONAL"
    UNCATEGORIZED = "UNCATEGORIZED"


@dataclass
class APIRecommendation:
    name: str
    category: str
    description: str
    use_case: str
    relevance_score: float


@dataclass
class ArticleResult:
    url: str
    url_hash: str
    title: str
    author: Optional[str]
    publication_date: Optional[str]
    domain: str
    content_markdown: str
    content_text: str
    word_count: int
    extraction_source: str
    life_os_domain: str
    life_os_summary: str
    key_insights: List[str]
    actionable_tasks: List[str]
    relevance_score: int
    api_recommendations: List[Dict[str, Any]]
    timestamp: str
    error: Optional[str] = None
    metadata: Optional[Dict] = None


# =============================================================================
# JS DETECTION
# =============================================================================

def detect_js_rendered(html: str) -> bool:
    """Detect if site requires JavaScript (React, Vue, Angular, etc.)"""
    if len(html) < 1000:  # Very small HTML
        return True
    
    # React/Vue/Angular indicators
    js_frameworks = [
        '<div id="root"></div>',
        '<div id="app"></div>',
        '<app-root>',
        '__NEXT_DATA__',
        'window.__NUXT__',
    ]
    
    for indicator in js_frameworks:
        if indicator in html:
            soup = BeautifulSoup(html, 'html.parser')
            body = soup.find('body')
            if body and len(body.get_text(strip=True)) < 500:
                return True
    
    return False


# =============================================================================
# STRATEGY 1A: JINA AI (FREE)
# =============================================================================

async def extract_with_jina_ai(url: str) -> Tuple[Optional[Dict], Optional[str]]:
    """Jina AI Reader - FREE, no API key needed"""
    try:
        print(f"üéØ Strategy 1A: Jina AI Reader...")
        
        jina_url = f"https://r.jina.ai/{url}"
        headers = {
            'Accept': 'application/json',
            'X-Return-Format': 'markdown'
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(jina_url, headers=headers, follow_redirects=True)
            response.raise_for_status()
            
            result = response.json()
            data = result.get('data', {})
            if not data:
                data = result
            
            title = data.get('title', 'Untitled')
            content_markdown = data.get('content', '')
            
            # Check if content too short (likely JS-rendered)
            if len(content_markdown) < 200:
                return None, "Content too short - likely JS-rendered site"
            
            h = html2text.HTML2Text()
            h.ignore_links = True
            h.ignore_images = True
            h.body_width = 0
            content_text = h.handle(content_markdown)
            
            return {
                'title': title,
                'author': data.get('author') or data.get('byline'),
                'date': data.get('publishedTime') or data.get('date'),
                'domain': urlparse(url).netloc,
                'text': content_text,
                'markdown': content_markdown,
                'description': data.get('description'),
            }, None
            
    except Exception as e:
        return None, f"Jina AI error: {str(e)}"


# =============================================================================
# STRATEGY 1B: PLAYWRIGHT MCP (NEW!)
# =============================================================================

async def extract_with_playwright_mcp(url: str) -> Tuple[Optional[Dict], Optional[str]]:
    """
    Playwright MCP - Uses your existing MCP infrastructure.
    Handles JS-rendered sites like PathForge (React SPAs).
    
    MCP Files: mcp_nodes.py, .mcp.json, orchestrator_mcp.yml
    """
    try:
        print(f"üéØ Strategy 1B: Playwright MCP (JS-rendered)...")
        
        # Import your existing MCP infrastructure
        try:
            from mcp_nodes import playwright_fetch
        except ImportError:
            # Fallback: Try to use MCP client directly
            try:
                import subprocess
                import tempfile
                
                # Create MCP request
                mcp_request = {
                    "jsonrpc": "2.0",
                    "method": "tools/call",
                    "params": {
                        "name": "playwright_navigate",
                        "arguments": {
                            "url": url
                        }
                    },
                    "id": 1
                }
                
                # Call MCP via subprocess (works with existing .mcp.json config)
                result = subprocess.run(
                    ['npx', '@modelcontextprotocol/server-playwright'],
                    input=json.dumps(mcp_request),
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                
                if result.returncode != 0:
                    return None, f"MCP error: {result.stderr}"
                
                mcp_response = json.loads(result.stdout)
                html_content = mcp_response.get('result', {}).get('content', '')
                
            except Exception as e:
                return None, f"MCP not available: {str(e)}"
        else:
            # Use your existing mcp_nodes.py function
            result = await playwright_fetch(url)
            html_content = result.get('html', '')
        
        if not html_content or len(html_content) < 200:
            return None, "No content returned from MCP"
        
        # Parse HTML content
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract title
        title_tag = soup.find('title')
        title = title_tag.get_text().strip() if title_tag else 'Untitled'
        
        # Extract metadata
        author = None
        author_meta = soup.find('meta', {'name': 'author'}) or \
                     soup.find('meta', {'property': 'article:author'})
        if author_meta:
            author = author_meta.get('content')
        
        date = None
        date_meta = soup.find('meta', {'property': 'article:published_time'}) or \
                   soup.find('time')
        if date_meta:
            date = date_meta.get('datetime') or date_meta.get('content') or date_meta.get_text()
        
        # Remove unwanted elements
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe']):
            tag.decompose()
        
        # Find main content
        main = soup.find('article') or soup.find('main') or \
               soup.find('div', class_=re.compile('content|article|post|blog')) or \
               soup.find('body')
        
        if not main:
            return None, "Could not find main content"
        
        # Convert to markdown
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = False
        h.body_width = 0
        markdown = h.handle(str(main))
        
        # Extract plain text
        text = main.get_text(separator='\n', strip=True)
        
        # Get description
        desc_meta = soup.find('meta', {'name': 'description'}) or \
                   soup.find('meta', {'property': 'og:description'})
        description = desc_meta.get('content') if desc_meta else None
        
        return {
            'title': title,
            'author': author,
            'date': date,
            'domain': urlparse(url).netloc,
            'text': text,
            'markdown': markdown,
            'description': description,
        }, None
        
    except Exception as e:
        return None, f"Playwright MCP error: {str(e)}"


# =============================================================================
# STRATEGY 2: TRAFILATURA (FREE)
# =============================================================================

async def extract_with_trafilatura(url: str) -> Tuple[Optional[Dict], Optional[str]]:
    """Trafilatura - FREE local library for simple sites"""
    try:
        import trafilatura
        print(f"üéØ Strategy 2: Trafilatura...")
        
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return None, "Failed to download"
        
        # Check if JS-rendered
        if detect_js_rendered(downloaded):
            return None, "JS-rendered site - use Playwright MCP"
        
        result = trafilatura.extract(
            downloaded,
            include_comments=False,
            include_tables=True,
            output_format='json',
            with_metadata=True
        )
        
        if not result:
            return None, "No content extracted"
        
        data = json.loads(result) if isinstance(result, str) else result
        markdown = trafilatura.extract(downloaded, output_format='markdown')
        
        return {
            'title': data.get('title', 'Untitled'),
            'author': data.get('author'),
            'date': data.get('date'),
            'domain': data.get('hostname', urlparse(url).netloc),
            'text': data.get('text', ''),
            'markdown': markdown or data.get('text', ''),
            'description': data.get('description'),
        }, None
        
    except ImportError:
        return None, "Trafilatura not installed"
    except Exception as e:
        return None, f"Trafilatura error: {str(e)}"


# =============================================================================
# STRATEGY 3: BEAUTIFULSOUP (FREE)
# =============================================================================

async def extract_with_beautifulsoup(url: str) -> Tuple[Optional[Dict], Optional[str]]:
    """BeautifulSoup - FREE basic fallback"""
    try:
        print(f"üéØ Strategy 3: BeautifulSoup...")
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url, follow_redirects=True)
            response.raise_for_status()
        
        if detect_js_rendered(response.text):
            return None, "JS-rendered site - use Playwright MCP"
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        title = soup.find('title')
        title = title.get_text().strip() if title else 'Untitled'
        
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            tag.decompose()
        
        main = soup.find('article') or soup.find('main') or soup.find('body')
        if not main:
            return None, "No main content"
        
        h = html2text.HTML2Text()
        h.body_width = 0
        markdown = h.handle(str(main))
        text = main.get_text(separator='\n', strip=True)
        
        return {
            'title': title,
            'author': None,
            'date': None,
            'domain': urlparse(url).netloc,
            'text': text,
            'markdown': markdown,
        }, None
        
    except Exception as e:
        return None, f"BeautifulSoup error: {str(e)}"


# =============================================================================
# STRATEGY 4: FIRECRAWL (PAID)
# =============================================================================

async def extract_with_firecrawl(url: str) -> Tuple[Optional[Dict], Optional[str]]:
    """Firecrawl - PAID $16/mo fallback"""
    api_key = os.getenv('FIRECRAWL_API_KEY')
    if not api_key:
        return None, "FIRECRAWL_API_KEY not set"
    
    try:
        print(f"üí∞ Strategy 4: Firecrawl (PAID)...")
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                'https://api.firecrawl.dev/v0/scrape',
                headers={'Authorization': f'Bearer {api_key}'},
                json={'url': url, 'formats': ['markdown']}
            )
            response.raise_for_status()
            
            data = response.json()
            if not data.get('success'):
                return None, f"Firecrawl failed: {data.get('error')}"
            
            content = data.get('data', {})
            return {
                'title': content.get('metadata', {}).get('title', 'Untitled'),
                'author': content.get('metadata', {}).get('author'),
                'date': content.get('metadata', {}).get('publishedTime'),
                'domain': urlparse(url).netloc,
                'text': content.get('markdown', ''),
                'markdown': content.get('markdown', ''),
            }, None
            
    except Exception as e:
        return None, f"Firecrawl error: {str(e)}"


# =============================================================================
# LIFE OS ANALYSIS (same as V2.1)
# =============================================================================

def load_api_mega_library() -> Dict[str, Any]:
    """Load API_MEGA_LIBRARY.md"""
    # Same as V2.1 - omitted for brevity
    return {}

def find_relevant_apis(content: str, title: str, domain: str, api_db: Dict) -> List[APIRecommendation]:
    """Find relevant APIs"""
    # Same as V2.1 - omitted for brevity
    return []

def analyze_for_life_os_v2(content: str, title: str, url: str, metadata: Dict, api_db: Dict) -> Dict[str, Any]:
    """Life OS domain analysis"""
    domain_keywords = {
        'BUSINESS': {
            'primary': ['startup', 'business', 'founder', 'company', 'validation', 'product'],
            'tech': ['saas', 'platform', 'api', 'software']
        },
        'MICHAEL': {'primary': ['swimming', 'swim', 'd1']},
        'FAMILY': {'primary': ['family', 'shabbat', 'kosher']},
        'PERSONAL': {'primary': ['adhd', 'productivity']}
    }
    
    text = f"{title} {content}".lower()
    domain_scores = {}
    
    for domain, categories in domain_keywords.items():
        score = 0
        for category, keywords in categories.items():
            weight = 2.0 if category == 'primary' else 1.0
            score += sum(1 for k in keywords if k in text) * weight
        domain_scores[domain] = score
    
    primary_domain = max(domain_scores, key=domain_scores.get)
    max_score = domain_scores[primary_domain]
    
    if max_score == 0:
        primary_domain = 'UNCATEGORIZED'
        max_score = 0
    
    sentences = re.split(r'[.!?]\s+', content)
    summary = '. '.join(sentences[:3]) + '.'
    
    insights = []
    for line in content.split('\n'):
        line = line.strip()
        if line and (line[0].isdigit() or line.startswith(('-', '‚Ä¢', '*'))):
            insight = re.sub(r'^[0-9\-\‚Ä¢\*\.\)\]\s]+', '', line).strip()
            if len(insight) > 10:
                insights.append(insight)
            if len(insights) >= 5:
                break
    
    tasks = []
    actionable_verbs = ['validate', 'test', 'build', 'create', 'implement']
    for sentence in sentences:
        if any(v in sentence.lower() for v in actionable_verbs):
            tasks.append(sentence.strip())
            if len(tasks) >= 3:
                break
    
    relevance = min(10, max(1, int(max_score))) if max_score > 0 else 1
    
    return {
        'domain': primary_domain,
        'domain_scores': domain_scores,
        'summary': summary,
        'insights': insights if insights else ['No specific insights extracted'],
        'tasks': tasks if tasks else ['No actionable tasks identified'],
        'relevance_score': relevance,
        'api_recommendations': []
    }


# =============================================================================
# MAIN EXTRACTION (V2.2)
# =============================================================================

def generate_url_hash(url: str) -> str:
    return hashlib.md5(url.encode()).hexdigest()[:12]


async def extract_article_v2_2(url: str) -> ArticleResult:
    """
    V2.2 extraction with MCP Playwright support for JS-rendered sites.
    """
    url = url.strip()
    url_hash = generate_url_hash(url)
    
    print(f"\nüì∞ Extracting V2.2 (MCP-ENABLED): {url}")
    print(f"   Hash: {url_hash}")
    
    api_db = load_api_mega_library()
    
    # Strategy 1A: Jina AI (FREE, fast)
    content, error = await extract_with_jina_ai(url)
    source = ArticleSource.JINA_AI.value
    
    # Strategy 1B: Playwright MCP if Jina failed or minimal content
    if not content or (error and "too short" in error.lower()):
        if MCP_AVAILABLE:
            print(f"   ‚ö†Ô∏è Jina AI: {error}")
            content, error = await extract_with_playwright_mcp(url)
            source = ArticleSource.PLAYWRIGHT_MCP.value
    
    # Strategy 2: Trafilatura
    if not content:
        print(f"   ‚ö†Ô∏è Playwright MCP: {error}")
        content, error = await extract_with_trafilatura(url)
        source = ArticleSource.TRAFILATURA.value
    
    # Strategy 3: BeautifulSoup
    if not content:
        print(f"   ‚ö†Ô∏è Trafilatura: {error}")
        content, error = await extract_with_beautifulsoup(url)
        source = ArticleSource.BEAUTIFULSOUP.value
    
    # Strategy 4: Firecrawl (PAID)
    if not content:
        print(f"   ‚ö†Ô∏è BeautifulSoup: {error}")
        content, error = await extract_with_firecrawl(url)
        source = ArticleSource.FIRECRAWL.value
    
    # All failed
    if not content:
        return ArticleResult(
            url=url, url_hash=url_hash, title="Extraction Failed",
            author=None, publication_date=None, domain=urlparse(url).netloc,
            content_markdown="", content_text="", word_count=0,
            extraction_source=ArticleSource.FAILED.value,
            life_os_domain=LifeOSDomain.UNCATEGORIZED.value,
            life_os_summary="", key_insights=[], actionable_tasks=[],
            relevance_score=0, api_recommendations=[],
            timestamp=datetime.now(timezone.utc).isoformat(), error=error
        )
    
    print(f"   ‚úÖ Extracted with {source}")
    print(f"   üìä Analyzing for Life OS...")
    
    analysis = analyze_for_life_os_v2(content['text'], content['title'], url, content, api_db)
    word_count = len(content['text'].split())
    
    result = ArticleResult(
        url=url, url_hash=url_hash, title=content['title'],
        author=content.get('author'), publication_date=content.get('date'),
        domain=content['domain'], content_markdown=content['markdown'],
        content_text=content['text'], word_count=word_count,
        extraction_source=source, life_os_domain=analysis['domain'],
        life_os_summary=analysis['summary'], key_insights=analysis['insights'],
        actionable_tasks=analysis['tasks'], relevance_score=analysis['relevance_score'],
        api_recommendations=analysis['api_recommendations'],
        timestamp=datetime.now(timezone.utc).isoformat(),
        metadata={'domain_scores': analysis['domain_scores']}
    )
    
    print(f"   ‚úÖ Domain: {analysis['domain']} ({analysis['relevance_score']}/10)")
    print(f"   ‚úÖ Words: {word_count}")
    
    return result


# =============================================================================
# CLI
# =============================================================================

if __name__ == "__main__":
    import asyncio
    import sys
    
    async def test():
        if len(sys.argv) < 2:
            print("Usage: python article_parser_agent_v2_2.py <url>")
            sys.exit(1)
        
        result = await extract_article_v2_2(sys.argv[1])
        
        print("\n" + "="*80)
        print("EXTRACTION RESULT V2.2")
        print("="*80)
        print(f"Title: {result.title}")
        print(f"Words: {result.word_count}")
        print(f"Source: {result.extraction_source}")
        print(f"Domain: {result.life_os_domain} ({result.relevance_score}/10)")
        print(f"\nFirst 500 chars:\n{result.content_text[:500]}...")
        print(f"\nüí∞ Cost: $0 (100% FREE)")
    
    asyncio.run(test())
