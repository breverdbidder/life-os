name: üì∫ YouTube Transcript Agent (Production)

on:
  repository_dispatch:
    types: [youtube_transcript]
  workflow_dispatch:
    inputs:
      video_url:
        description: 'YouTube URL (videos, shorts, reels)'
        required: true
        default: 'https://youtube.com/shorts/C2Dl6P7diHw'
      whisper_model:
        description: 'Whisper model (for audio fallback)'
        required: false
        default: 'base'
        type: choice
        options: [tiny, base, small]
      category:
        description: 'Category for Supabase'
        required: false
        default: 'learning'
        type: choice
        options: [learning, michael_swim, business, personal, research, thesys]
      skip_to_apify:
        description: 'Skip to Apify directly (for shorts)'
        required: false
        default: 'false'
        type: choice
        options: ['false', 'true']

env:
  SUPABASE_URL: https://mocerqjnksmhcjzxrewo.supabase.co

jobs:
  transcribe:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: üì¶ Install Dependencies
        run: |
          pip install youtube-transcript-api yt-dlp httpx requests openai-whisper
          sudo apt-get update && sudo apt-get install -y ffmpeg
          
      - name: üì∫ Transcribe (4-Tier Strategy)
        id: transcribe
        env:
          VIDEO_URL: ${{ github.event.inputs.video_url || github.event.client_payload.video_url }}
          WHISPER_MODEL: ${{ github.event.inputs.whisper_model }}
          CATEGORY: ${{ github.event.inputs.category || github.event.client_payload.category || 'learning' }}
          SKIP_TO_APIFY: ${{ github.event.inputs.skip_to_apify }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
        run: |
          python3 << 'PYEOF'
          import os, re, json, subprocess, glob, requests, time
          from datetime import datetime, timezone
          
          video_url = os.environ.get('VIDEO_URL', '')
          whisper_model = os.environ.get('WHISPER_MODEL', 'base')
          category = os.environ.get('CATEGORY', 'learning')
          skip_to_apify = os.environ.get('SKIP_TO_APIFY', 'false') == 'true'
          supabase_url = os.environ.get('SUPABASE_URL', '')
          supabase_key = os.environ.get('SUPABASE_KEY', '')
          apify_token = os.environ.get('APIFY_API_TOKEN', '')
          
          print(f"{'='*60}")
          print(f"üì∫ YOUTUBE TRANSCRIPT V7 (4-Tier with Apify)")
          print(f"{'='*60}")
          
          # Extract video ID - supports all formats
          video_id = None
          patterns = [
              r'shorts/([a-zA-Z0-9_-]{11})',
              r'(?:v=|/v/)([a-zA-Z0-9_-]{11})',
              r'youtu\.be/([a-zA-Z0-9_-]{11})',
              r'embed/([a-zA-Z0-9_-]{11})',
              r'/live/([a-zA-Z0-9_-]{11})',
              r'watch\?.*v=([a-zA-Z0-9_-]{11})'
          ]
          for p in patterns:
              m = re.search(p, video_url)
              if m:
                  video_id = m.group(1)
                  break
          
          if not video_id:
              print("‚ùå Invalid YouTube URL")
              exit(1)
          
          # Detect video type
          video_type = "short" if '/shorts/' in video_url else "live" if '/live/' in video_url else "regular"
          is_short = video_type == "short"
          print(f"Video: {video_id} ({video_type})")
          print(f"Skip to Apify: {skip_to_apify}")
          
          # Get metadata via yt-dlp
          metadata = {"title": "Unknown", "channel": "Unknown", "duration": 0}
          try:
              r = subprocess.run(['yt-dlp', '--dump-json', '--no-download', video_url], 
                                capture_output=True, text=True, timeout=30)
              if r.returncode == 0:
                  d = json.loads(r.stdout)
                  metadata = {
                      "title": d.get('title', ''),
                      "channel": d.get('channel', '') or d.get('uploader', ''),
                      "duration": d.get('duration', 0)
                  }
                  print(f"Title: {metadata['title'][:60]}")
                  print(f"Channel: {metadata['channel']}")
                  print(f"Duration: {metadata['duration']}s")
          except Exception as e:
              print(f"‚ö†Ô∏è Metadata error: {e}")
          
          transcript = None
          source = "none"
          
          # For shorts, Apify works best - optionally skip to it
          if skip_to_apify and apify_token:
              print("
‚ö° Skipping to Apify (shorts optimization)")
          else:
              # TIER 1: YouTube Transcript API (FREE)
              print("
üéØ TIER 1: YouTube Transcript API (FREE)")
              try:
                  from youtube_transcript_api import YouTubeTranscriptApi
                  fetched = YouTubeTranscriptApi().fetch(video_id)
                  transcript = ' '.join([e.text for e in fetched])
                  source = "youtube_api"
                  print(f"‚úÖ {len(transcript)} chars via youtube_transcript_api")
              except Exception as e:
                  print(f"‚ö†Ô∏è No captions available: {str(e)[:100]}")
              
              # TIER 2: yt-dlp auto-subtitles (FREE)
              if not transcript or len(transcript) < 20:
                  print("
üéØ TIER 2: yt-dlp subtitles (FREE)")
                  try:
                      subprocess.run([
                          'yt-dlp', '--skip-download', 
                          '--write-auto-sub', '--sub-lang', 'en',
                          '-o', f'/tmp/{video_id}', video_url
                      ], capture_output=True, timeout=60)
                      
                      vtt_files = glob.glob(f'/tmp/{video_id}*.vtt')
                      if vtt_files:
                          with open(vtt_files[0]) as f:
                              lines = [l.strip() for l in f 
                                      if l.strip() and not any(x in l for x in ['WEBVTT', '-->', '<'])]
                          transcript = ' '.join(lines)
                          source = "yt-dlp_subs"
                          print(f"‚úÖ {len(transcript)} chars via yt-dlp subtitles")
                      else:
                          print("‚ö†Ô∏è No subtitle files found")
                  except Exception as e:
                      print(f"‚ö†Ô∏è yt-dlp subs error: {e}")
              
              # TIER 3: Whisper audio transcription (FREE but slow)
              if not transcript or len(transcript) < 20:
                  # Skip Whisper for shorts - Apify is faster and works better
                  if is_short:
                      print("
‚è≠Ô∏è Skipping Whisper for short - Apify is faster")
                  else:
                      print(f"
üéØ TIER 3: Whisper ({whisper_model}) (FREE)")
                      try:
                          print("   Downloading audio...")
                          os.makedirs('/tmp/audio', exist_ok=True)
                          
                          result = subprocess.run([
                              'yt-dlp', '-x', '--audio-format', 'mp3',
                              '-o', '/tmp/audio/%(id)s.%(ext)s', video_url
                          ], capture_output=True, text=True, timeout=120)
                          
                          audio_files = glob.glob(f'/tmp/audio/{video_id}*')
                          actual_audio = next((f for f in audio_files 
                                              if any(f.endswith(e) for e in ['.mp3','.m4a','.webm','.opus'])), None)
                          
                          if actual_audio and os.path.exists(actual_audio):
                              size = os.path.getsize(actual_audio)
                              print(f"   Audio: {actual_audio} ({size} bytes)")
                              
                              if size > 1000:
                                  print(f"   Loading Whisper {whisper_model}...")
                                  import whisper
                                  model = whisper.load_model(whisper_model)
                                  print("   Transcribing...")
                                  result = model.transcribe(actual_audio)
                                  transcript = result["text"].strip()
                                  source = f"whisper_{whisper_model}"
                                  print(f"‚úÖ {len(transcript)} chars via Whisper")
                          else:
                              print("‚ö†Ô∏è No audio file found")
                      except Exception as e:
                          print(f"‚ö†Ô∏è Whisper error: {e}")
          
          # TIER 4: Apify (PAID - ~$0.002/video)
          if (not transcript or len(transcript) < 20) and apify_token:
              print("
üéØ TIER 4: Apify karamelo/youtube-transcripts (~$0.002)")
              try:
                  # Start the actor run
                  run_response = requests.post(
                      f"https://api.apify.com/v2/acts/karamelo~youtube-transcripts/runs?token={apify_token}&waitForFinish=120",
                      headers={"Content-Type": "application/json"},
                      json={
                          "urls": [video_url],
                          "output_format": "captions_text"
                      },
                      timeout=130
                  )
                  
                  if run_response.status_code == 201:
                      run_data = run_response.json()
                      dataset_id = run_data.get('data', {}).get('defaultDatasetId')
                      run_status = run_data.get('data', {}).get('status')
                      
                      print(f"   Run status: {run_status}")
                      
                      # Wait for completion if not done
                      if run_status not in ['SUCCEEDED', 'FINISHED']:
                          print("   Waiting for completion...")
                          time.sleep(10)
                      
                      # Get results from dataset
                      if dataset_id:
                          items_response = requests.get(
                              f"https://api.apify.com/v2/datasets/{dataset_id}/items?token={apify_token}",
                              timeout=30
                          )
                          
                          if items_response.status_code == 200:
                              items = items_response.json()
                              if items and len(items) > 0:
                                  captions = items[0].get('captions', [])
                                  if captions:
                                      # Clean HTML entities
                                      raw_text = ' '.join(captions)
                                      transcript = raw_text.replace('&#39;', "'").replace('&amp;', '&').replace('&quot;', '"')
                                      source = "apify_karamelo"
                                      
                                      # Update metadata from Apify if we didn't get it before
                                      if metadata['title'] == 'Unknown':
                                          metadata['title'] = items[0].get('title', 'Unknown')
                                          metadata['channel'] = items[0].get('channelName', 'Unknown')
                                      
                                      print(f"‚úÖ {len(transcript)} chars via Apify")
                                  else:
                                      print("‚ö†Ô∏è Apify returned no captions")
                              else:
                                  print("‚ö†Ô∏è Apify dataset empty")
                          else:
                              print(f"‚ö†Ô∏è Apify dataset error: {items_response.status_code}")
                      else:
                          print("‚ö†Ô∏è No dataset ID returned")
                  else:
                      print(f"‚ö†Ô∏è Apify run failed: {run_response.status_code}")
                      print(f"   {run_response.text[:200]}")
              except Exception as e:
                  print(f"‚ö†Ô∏è Apify error: {e}")
          
          # Final result
          if not transcript or len(transcript) < 20:
              transcript = "Transcript unavailable - all tiers failed"
              source = "failed"
          
          print(f"
{'='*60}")
          print(f"RESULT: {source} | {len(transcript)} chars | {len(transcript.split())} words")
          print(f"{'='*60}")
          print(f"
{transcript[:500]}{'...' if len(transcript) > 500 else ''}")
          
          # Build output
          output = {
              "video_id": video_id,
              "video_url": video_url,
              "video_type": video_type,
              "title": metadata['title'],
              "channel": metadata['channel'],
              "duration_seconds": metadata.get('duration', 0),
              "transcript": transcript,
              "transcript_length": len(transcript),
              "word_count": len(transcript.split()),
              "transcript_source": source,
              "tier_used": {
                  "youtube_api": 1,
                  "yt-dlp_subs": 2,
                  "whisper_base": 3, "whisper_tiny": 3, "whisper_small": 3,
                  "apify_karamelo": 4,
                  "failed": 0
              }.get(source, 0),
              "category": category,
              "extracted_at": datetime.now(timezone.utc).isoformat(),
              "workflow_version": "v7"
          }
          
          # Save to file
          with open('transcript.json', 'w') as f:
              json.dump(output, f, indent=2)
          
          # Save to Supabase
          if supabase_key and source != "failed":
              try:
                  headers = {
                      "apikey": supabase_key,
                      "Authorization": f"Bearer {supabase_key}",
                      "Content-Type": "application/json",
                      "Prefer": "return=minimal"
                  }
                  
                  tier_emoji = {1: "üÜì", 2: "üÜì", 3: "üÜì", 4: "üí∞"}.get(output['tier_used'], "‚ùì")
                  
                  insight = {
                      "user_id": 1,
                      "insight_type": "youtube_transcript",
                      "title": f"üì∫ {metadata['title'][:80]}",
                      "content": transcript[:10000],
                      "category": category,
                      "source": f"youtube_v7_{source}",
                      "priority": 2,
                      "status": "Active",
                      "metadata": json.dumps({
                          "video_id": video_id,
                          "video_type": video_type,
                          "channel": metadata['channel'],
                          "duration": metadata['duration'],
                          "source": source,
                          "tier": output['tier_used'],
                          "word_count": output['word_count']
                      })
                  }
                  
                  r = requests.post(
                      f"{supabase_url}/rest/v1/insights",
                      headers=headers,
                      json=insight,
                      timeout=10
                  )
                  if r.status_code in [200, 201]:
                      print(f"
‚úÖ Saved to Supabase (insights) {tier_emoji}")
                  else:
                      print(f"
‚ö†Ô∏è Supabase: {r.status_code}")
              except Exception as e:
                  print(f"
‚ö†Ô∏è Supabase error: {e}")
          
          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"video_id={video_id}
")
              f.write(f"source={source}
")
              f.write(f"tier={output['tier_used']}
")
              f.write(f"length={len(transcript)}
")
              f.write(f"success={'true' if source != 'failed' else 'false'}
")
          PYEOF
          
      - name: üìä Summary
        run: |
          echo "## üì∫ YouTube Transcript V7 Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Video ID | ${{ steps.transcribe.outputs.video_id }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Source | ${{ steps.transcribe.outputs.source }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Tier | ${{ steps.transcribe.outputs.tier }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Length | ${{ steps.transcribe.outputs.length }} chars |" >> $GITHUB_STEP_SUMMARY
          echo "| Success | ${{ steps.transcribe.outputs.success }} |" >> $GITHUB_STEP_SUMMARY
          
      - uses: actions/upload-artifact@v4
        with:
          name: transcript-${{ steps.transcribe.outputs.video_id }}
          path: transcript.json
          retention-days: 30

# Last refresh: 2025-12-22T12:16:08Z
