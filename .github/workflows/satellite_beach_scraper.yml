name: Satellite Beach SwimCloud Scraper

on:
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Playwright
        run: |
          pip install playwright beautifulsoup4
          playwright install chromium
          playwright install-deps chromium
      
      - name: Scrape Satellite Beach Swimmers
        run: |
          python3 << 'PYTHON_EOF'
          import asyncio
          import json
          import re
          from datetime import datetime
          from playwright.async_api import async_playwright

          # Satellite Beach High School Swimmers - ALL 5
          SWIMMERS = {
              "3250085": {"name": "Michael Shapira", "class": 2027},
              "2928537": {"name": "Bastian Soto", "class": 2027},
              "1518102": {"name": "Gavin Domboru", "class": 2027},
              "1754554": {"name": "Max Morgan", "class": 2026},
              "2662363": {"name": "Sawyer Davis", "class": 2026, "note": "Ohio State Commit, 2A State Champion"}
          }

          def parse_time_to_seconds(time_str):
              """Convert time string to seconds."""
              try:
                  if ':' in time_str:
                      parts = time_str.split(':')
                      return float(parts[0]) * 60 + float(parts[1])
                  return float(time_str)
              except:
                  return None

          async def scrape_swimmer_times(page, swimmer_id, swimmer_info):
              """Scrape all times for a swimmer."""
              url = f"https://www.swimcloud.com/swimmer/{swimmer_id}/"
              print(f"\n{'='*60}")
              print(f"Scraping: {swimmer_info['name']} (ID: {swimmer_id})")
              print(f"URL: {url}")
              
              try:
                  await page.goto(url, wait_until="networkidle", timeout=60000)
                  await page.wait_for_timeout(5000)
                  
                  data = {
                      "swimmer_id": swimmer_id,
                      "name": swimmer_info["name"],
                      "class_of": swimmer_info.get("class"),
                      "school": "Satellite Sr High School",
                      "team": "Swim Melbourne (MELB-FL)",
                      "note": swimmer_info.get("note", ""),
                      "scraped_at": datetime.now().isoformat(),
                      "profile": {},
                      "personal_bests": {}
                  }
                  
                  body_text = await page.locator("body").text_content()
                  
                  # Extract Power Index
                  pi_match = re.search(r'Power\s*index\s*(\d+\.?\d*)', body_text, re.I)
                  if pi_match:
                      data["profile"]["power_index"] = float(pi_match.group(1))
                      print(f"  Power Index: {data['profile']['power_index']}")
                  
                  # Extract Florida Rank
                  fl_rank = re.search(r'Florida\s*rank\s*(\d+)', body_text, re.I)
                  if fl_rank:
                      data["profile"]["florida_rank"] = int(fl_rank.group(1))
                      print(f"  Florida Rank: {data['profile']['florida_rank']}")
                  
                  # Extract Class Year from page
                  class_match = re.search(r'Class\s*[¬∑:\s]*(\d{4})', body_text)
                  if class_match:
                      data["profile"]["class_of"] = int(class_match.group(1))
                  
                  html = await page.content()
                  
                  # Time patterns - improved to catch more formats
                  time_patterns = [
                      (r'50\s*Y(?:ard)?\s*Free(?:style)?[^\d]{0,50}?(\d{2}\.\d{2})', '50 Free'),
                      (r'100\s*Y(?:ard)?\s*Free(?:style)?[^\d]{0,50}?(\d{2}\.\d{2}|\d:\d{2}\.\d{2})', '100 Free'),
                      (r'200\s*Y(?:ard)?\s*Free(?:style)?[^\d]{0,50}?(\d:\d{2}\.\d{2})', '200 Free'),
                      (r'500\s*Y(?:ard)?\s*Free(?:style)?[^\d]{0,50}?(\d:\d{2}\.\d{2})', '500 Free'),
                      (r'50\s*Y(?:ard)?\s*Fly[^\d]{0,50}?(\d{2}\.\d{2})', '50 Fly'),
                      (r'100\s*Y(?:ard)?\s*Fly[^\d]{0,50}?(\d{2}\.\d{2}|\d:\d{2}\.\d{2})', '100 Fly'),
                      (r'50\s*Y(?:ard)?\s*Back[^\d]{0,50}?(\d{2}\.\d{2})', '50 Back'),
                      (r'100\s*Y(?:ard)?\s*Back[^\d]{0,50}?(\d{2}\.\d{2}|\d:\d{2}\.\d{2})', '100 Back'),
                      (r'200\s*Y(?:ard)?\s*Back[^\d]{0,50}?(\d:\d{2}\.\d{2})', '200 Back'),
                      (r'50\s*Y(?:ard)?\s*Breast[^\d]{0,50}?(\d{2}\.\d{2})', '50 Breast'),
                      (r'100\s*Y(?:ard)?\s*Breast[^\d]{0,50}?(\d{2}\.\d{2}|\d:\d{2}\.\d{2})', '100 Breast'),
                      (r'200\s*Y(?:ard)?\s*Breast[^\d]{0,50}?(\d:\d{2}\.\d{2})', '200 Breast'),
                      (r'200\s*Y(?:ard)?\s*IM[^\d]{0,50}?(\d:\d{2}\.\d{2})', '200 IM'),
                      (r'400\s*Y(?:ard)?\s*IM[^\d]{0,50}?(\d:\d{2}\.\d{2})', '400 IM'),
                  ]
                  
                  for pattern, event_name in time_patterns:
                      matches = re.findall(pattern, html, re.I)
                      if matches:
                          time_str = matches[0]
                          seconds = parse_time_to_seconds(time_str)
                          if seconds and seconds > 10:  # Filter out spurious matches
                              data["personal_bests"][event_name] = {
                                  "time": time_str,
                                  "seconds": seconds
                              }
                  
                  # Also try the times page
                  try:
                      times_url = f"https://www.swimcloud.com/swimmer/{swimmer_id}/times/"
                      await page.goto(times_url, wait_until="networkidle", timeout=30000)
                      await page.wait_for_timeout(3000)
                      
                      html = await page.content()
                      for pattern, event_name in time_patterns:
                          matches = re.findall(pattern, html, re.I)
                          if matches and event_name not in data["personal_bests"]:
                              time_str = matches[0]
                              seconds = parse_time_to_seconds(time_str)
                              if seconds and seconds > 10:
                                  data["personal_bests"][event_name] = {
                                      "time": time_str,
                                      "seconds": seconds
                                  }
                  except Exception as e:
                      print(f"  Times page error: {e}")
                  
                  print(f"  Personal Bests Found: {len(data['personal_bests'])} events")
                  for event, time_data in sorted(data["personal_bests"].items()):
                      print(f"    {event}: {time_data['time']}")
                  
                  return data
                  
              except Exception as e:
                  print(f"Error scraping {swimmer_id}: {e}")
                  return {
                      "swimmer_id": swimmer_id,
                      "name": swimmer_info["name"],
                      "error": str(e)
                  }

          async def main():
              print("üèä Satellite Beach High School SwimCloud Scraper V2")
              print(f"   Scraping {len(SWIMMERS)} swimmers (including Sawyer Davis)")
              print("="*60)
              
              results = {}
              
              async with async_playwright() as p:
                  browser = await p.chromium.launch(
                      headless=True,
                      args=['--no-sandbox', '--disable-setuid-sandbox']
                  )
                  context = await browser.new_context(
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                      viewport={"width": 1920, "height": 1080}
                  )
                  page = await context.new_page()
                  
                  for swimmer_id, swimmer_info in SWIMMERS.items():
                      data = await scrape_swimmer_times(page, swimmer_id, swimmer_info)
                      results[swimmer_info["name"]] = data
                      await asyncio.sleep(3)
                  
                  await browser.close()
              
              output = {
                  "scraped_at": datetime.now().isoformat(),
                  "school": "Satellite Beach High School",
                  "team": "Swim Melbourne (MELB-FL)",
                  "swimmer_count": len(SWIMMERS),
                  "swimmers": results
              }
              
              with open("satellite_beach_swimmers.json", "w") as f:
                  json.dump(output, f, indent=2)
              
              print("\n" + "="*60)
              print("SATELLITE BEACH HIGH SCHOOL - FINAL SUMMARY")
              print("="*60)
              for name, data in results.items():
                  pbs = data.get("personal_bests", {})
                  pi = data.get('profile', {}).get('power_index', 'N/A')
                  fl_rank = data.get('profile', {}).get('florida_rank', 'N/A')
                  note = data.get('note', '')
                  print(f"\n{name} (Class {data.get('class_of', 'N/A')})")
                  if note:
                      print(f"  ‚≠ê {note}")
                  print(f"  Power Index: {pi} | FL Rank: {fl_rank}")
                  print(f"  Events: {len(pbs)}")
                  for event in ['50 Free', '100 Free', '200 Free', '50 Fly', '100 Fly', '100 Back', '100 Breast']:
                      if event in pbs:
                          print(f"    {event}: {pbs[event]['time']}")
              
              print(f"\nüíæ Saved to satellite_beach_swimmers.json")
              return output

          if __name__ == "__main__":
              asyncio.run(main())
          PYTHON_EOF
      
      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: satellite-beach-swimmers-${{ github.run_number }}
          path: satellite_beach_swimmers.json
      
      - name: Commit to Repo
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          mkdir -p michael_d1_agents_v3/data
          cp satellite_beach_swimmers.json michael_d1_agents_v3/data/
          git add michael_d1_agents_v3/data/satellite_beach_swimmers.json
          git diff --staged --quiet || git commit -m "data: Satellite Beach 5 swimmers (incl Sawyer Davis) $(date +'%Y-%m-%d %H:%M')"
          git push || echo "Nothing to push"
      
      - name: Display Results
        run: cat satellite_beach_swimmers.json
