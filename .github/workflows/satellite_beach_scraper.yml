name: Satellite Beach SwimCloud Scraper

on:
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    env:
      APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: pip install requests
      
      - name: Scrape All Swimmers with Apify
        run: |
          python3 << 'PYTHON_EOF'
          import os
          import json
          import time
          import re
          import requests
          from datetime import datetime, timezone

          APIFY_TOKEN = os.environ.get('APIFY_API_TOKEN')
          SUPABASE_URL = os.environ.get('SUPABASE_URL', 'https://mocerqjnksmhcjzxrewo.supabase.co')
          SUPABASE_KEY = os.environ.get('SUPABASE_KEY')

          # ALL 5 Satellite Beach swimmers
          SWIMMERS = {
              "3250085": {"name": "Michael Shapira", "class": 2027},
              "2928537": {"name": "Bastian Soto", "class": 2027},
              "1518102": {"name": "Gavin Domboru", "class": 2027},
              "1754554": {"name": "Max Morgan", "class": 2026},
              "2662363": {"name": "Sawyer Davis", "class": 2026, "note": "Ohio State Commit, 2A State Champion"}
          }

          def run_apify_scraper_for_swimmer(swimmer_id, swimmer_info):
              """Run Apify Web Scraper for a single swimmer"""
              
              print(f"\n{'='*60}")
              print(f"ðŸŠ SCRAPING: {swimmer_info['name']} (ID: {swimmer_id})")
              print(f"{'='*60}")
              
              # URLs to scrape - profile AND times page
              urls = [
                  f"https://www.swimcloud.com/swimmer/{swimmer_id}/",
                  f"https://www.swimcloud.com/swimmer/{swimmer_id}/times/",
                  f"https://www.swimcloud.com/swimmer/{swimmer_id}/times/?page=1&course=Y"
              ]
              
              # Comprehensive pageFunction to extract ALL data
              page_function = '''
          async function pageFunction(context) {
              const { request, page, log } = context;
              log.info('Processing: ' + request.url);
              
              // Wait for full page load
              await page.waitForSelector('body', { timeout: 30000 });
              await page.waitForTimeout(5000);
              
              const data = await page.evaluate(() => {
                  const result = {
                      name: '',
                      power_index: null,
                      florida_rank: null,
                      class_of: null,
                      team: '',
                      times: {},
                      all_times_found: [],
                      raw_text: ''
                  };
                  
                  // Get full page text
                  const bodyText = document.body.innerText;
                  result.raw_text = bodyText.substring(0, 10000);
                  
                  // Extract swimmer name
                  const h1 = document.querySelector('h1');
                  if (h1) result.name = h1.textContent.trim();
                  
                  // Extract Power Index
                  const piMatch = bodyText.match(/Power\\s*index\\s*(\\d+\\.?\\d*)/i);
                  if (piMatch) result.power_index = parseFloat(piMatch[1]);
                  
                  // Extract Florida Rank
                  const flMatch = bodyText.match(/Florida\\s*rank\\s*(\\d+)/i);
                  if (flMatch) result.florida_rank = parseInt(flMatch[1]);
                  
                  // Extract Class Year
                  const classMatch = bodyText.match(/Class\\s*[Â·:]?\\s*(\\d{4})/i);
                  if (classMatch) result.class_of = parseInt(classMatch[1]);
                  
                  // Extract Team
                  const teamLink = document.querySelector('a[href*="/team/"]');
                  if (teamLink) result.team = teamLink.textContent.trim();
                  
                  // COMPREHENSIVE TIME EXTRACTION
                  // Pattern: Event name + time
                  const events = [
                      { name: '50 Free', pattern: /50\\s*(?:Y(?:ard)?)?\\s*Free[\\s\\S]{0,100}?(\\d{2}\\.\\d{2})/gi },
                      { name: '100 Free', pattern: /100\\s*(?:Y(?:ard)?)?\\s*Free[\\s\\S]{0,100}?(\\d{2}\\.\\d{2}|\\d:\\d{2}\\.\\d{2})/gi },
                      { name: '200 Free', pattern: /200\\s*(?:Y(?:ard)?)?\\s*Free[\\s\\S]{0,100}?(\\d:\\d{2}\\.\\d{2})/gi },
                      { name: '500 Free', pattern: /500\\s*(?:Y(?:ard)?)?\\s*Free[\\s\\S]{0,100}?(\\d:\\d{2}\\.\\d{2})/gi },
                      { name: '50 Fly', pattern: /50\\s*(?:Y(?:ard)?)?\\s*(?:Fly|Butterfly)[\\s\\S]{0,100}?(\\d{2}\\.\\d{2})/gi },
                      { name: '100 Fly', pattern: /100\\s*(?:Y(?:ard)?)?\\s*(?:Fly|Butterfly)[\\s\\S]{0,100}?(\\d{2}\\.\\d{2}|\\d:\\d{2}\\.\\d{2})/gi },
                      { name: '200 Fly', pattern: /200\\s*(?:Y(?:ard)?)?\\s*(?:Fly|Butterfly)[\\s\\S]{0,100}?(\\d:\\d{2}\\.\\d{2})/gi },
                      { name: '50 Back', pattern: /50\\s*(?:Y(?:ard)?)?\\s*Back[\\s\\S]{0,100}?(\\d{2}\\.\\d{2})/gi },
                      { name: '100 Back', pattern: /100\\s*(?:Y(?:ard)?)?\\s*Back[\\s\\S]{0,100}?(\\d{2}\\.\\d{2}|\\d:\\d{2}\\.\\d{2})/gi },
                      { name: '200 Back', pattern: /200\\s*(?:Y(?:ard)?)?\\s*Back[\\s\\S]{0,100}?(\\d:\\d{2}\\.\\d{2})/gi },
                      { name: '50 Breast', pattern: /50\\s*(?:Y(?:ard)?)?\\s*Breast[\\s\\S]{0,100}?(\\d{2}\\.\\d{2})/gi },
                      { name: '100 Breast', pattern: /100\\s*(?:Y(?:ard)?)?\\s*Breast[\\s\\S]{0,100}?(\\d{2}\\.\\d{2}|\\d:\\d{2}\\.\\d{2})/gi },
                      { name: '200 Breast', pattern: /200\\s*(?:Y(?:ard)?)?\\s*Breast[\\s\\S]{0,100}?(\\d:\\d{2}\\.\\d{2})/gi },
                      { name: '200 IM', pattern: /200\\s*(?:Y(?:ard)?)?\\s*(?:IM|Individual)[\\s\\S]{0,100}?(\\d:\\d{2}\\.\\d{2})/gi },
                      { name: '400 IM', pattern: /400\\s*(?:Y(?:ard)?)?\\s*(?:IM|Individual)[\\s\\S]{0,100}?(\\d:\\d{2}\\.\\d{2})/gi }
                  ];
                  
                  for (const event of events) {
                      const matches = [...bodyText.matchAll(event.pattern)];
                      if (matches.length > 0) {
                          // Get all times for this event
                          const timesFound = matches.map(m => m[1]).filter(t => {
                              // Validate time is reasonable
                              const secs = t.includes(':') ? 
                                  parseFloat(t.split(':')[0]) * 60 + parseFloat(t.split(':')[1]) :
                                  parseFloat(t);
                              return secs > 15 && secs < 600;
                          });
                          
                          if (timesFound.length > 0) {
                              // Convert to seconds and find best (lowest)
                              const withSeconds = timesFound.map(t => {
                                  const secs = t.includes(':') ? 
                                      parseFloat(t.split(':')[0]) * 60 + parseFloat(t.split(':')[1]) :
                                      parseFloat(t);
                                  return { time: t, seconds: secs };
                              });
                              
                              // Sort by seconds (fastest first)
                              withSeconds.sort((a, b) => a.seconds - b.seconds);
                              
                              result.times[event.name] = {
                                  time: withSeconds[0].time,
                                  seconds: withSeconds[0].seconds
                              };
                              
                              result.all_times_found.push({
                                  event: event.name,
                                  times: withSeconds.slice(0, 5)
                              });
                          }
                      }
                  }
                  
                  // Also try table extraction
                  const rows = document.querySelectorAll('tr');
                  rows.forEach(row => {
                      const text = row.textContent;
                      for (const event of events) {
                          if (!result.times[event.name]) {
                              const match = event.pattern.exec(text);
                              if (match) {
                                  const t = match[1];
                                  const secs = t.includes(':') ? 
                                      parseFloat(t.split(':')[0]) * 60 + parseFloat(t.split(':')[1]) :
                                      parseFloat(t);
                                  if (secs > 15 && secs < 600) {
                                      result.times[event.name] = { time: t, seconds: secs };
                                  }
                              }
                          }
                      }
                  });
                  
                  return result;
              });
              
              return {
                  url: request.url,
                  swimmer_id: request.url.match(/swimmer\\/(\\d+)/)?.[1],
                  ...data,
                  scraped_at: new Date().toISOString()
              };
          }
          '''
              
              scraper_input = {
                  "startUrls": [{"url": url} for url in urls],
                  "pageFunction": page_function,
                  "proxyConfiguration": {
                      "useApifyProxy": True,
                      "apifyProxyGroups": ["RESIDENTIAL"]
                  },
                  "maxConcurrency": 1,
                  "maxRequestsPerCrawl": 10,
                  "waitUntil": "networkidle",
                  "pageLoadTimeoutSecs": 60
              }
              
              print(f"ðŸ“¡ Starting Apify Web Scraper...")
              
              response = requests.post(
                  "https://api.apify.com/v2/acts/apify~web-scraper/runs",
                  headers={
                      "Authorization": f"Bearer {APIFY_TOKEN}",
                      "Content-Type": "application/json"
                  },
                  json=scraper_input,
                  timeout=60
              )
              
              if response.status_code != 201:
                  print(f"âŒ Failed to start: {response.status_code}")
                  return None
              
              run_data = response.json()
              run_id = run_data["data"]["id"]
              print(f"âœ… Run started: {run_id}")
              
              # Wait for completion
              max_wait = 180
              start = time.time()
              
              while time.time() - start < max_wait:
                  time.sleep(10)
                  
                  status_resp = requests.get(
                      f"https://api.apify.com/v2/actor-runs/{run_id}",
                      headers={"Authorization": f"Bearer {APIFY_TOKEN}"},
                      timeout=30
                  )
                  
                  status_data = status_resp.json()["data"]
                  status = status_data["status"]
                  elapsed = int(time.time() - start)
                  
                  print(f"   [{elapsed}s] {status}")
                  
                  if status in ["SUCCEEDED", "FAILED", "ABORTED", "TIMED-OUT"]:
                      break
              
              if status != "SUCCEEDED":
                  print(f"âŒ Run failed: {status}")
                  return None
              
              # Get results
              dataset_id = status_data["defaultDatasetId"]
              results = requests.get(
                  f"https://api.apify.com/v2/datasets/{dataset_id}/items",
                  headers={"Authorization": f"Bearer {APIFY_TOKEN}"},
                  timeout=30
              ).json()
              
              # Combine results from all pages
              combined = {
                  "swimmer_id": swimmer_id,
                  "name": swimmer_info["name"],
                  "class_of": swimmer_info.get("class"),
                  "note": swimmer_info.get("note", ""),
                  "profile": {},
                  "personal_bests": {}
              }
              
              for item in results:
                  if item.get("power_index"):
                      combined["profile"]["power_index"] = item["power_index"]
                  if item.get("florida_rank"):
                      combined["profile"]["florida_rank"] = item["florida_rank"]
                  if item.get("class_of"):
                      combined["profile"]["class_of"] = item["class_of"]
                  if item.get("team"):
                      combined["profile"]["team"] = item["team"]
                  
                  # Merge times - keep fastest
                  for event, time_data in item.get("times", {}).items():
                      if event not in combined["personal_bests"]:
                          combined["personal_bests"][event] = time_data
                      elif time_data["seconds"] < combined["personal_bests"][event]["seconds"]:
                          combined["personal_bests"][event] = time_data
              
              print(f"\n   Profile: PI={combined['profile'].get('power_index', 'N/A')}, FL Rank={combined['profile'].get('florida_rank', 'N/A')}")
              print(f"   Events: {len(combined['personal_bests'])}")
              for event, data in sorted(combined["personal_bests"].items()):
                  print(f"     {event}: {data['time']}")
              
              return combined

          def main():
              print("="*60)
              print("ðŸŠ SATELLITE BEACH APIFY PRO SCRAPER")
              print("   Using Apify Web Scraper with RESIDENTIAL proxies")
              print("="*60)
              
              if not APIFY_TOKEN:
                  print("âŒ APIFY_API_TOKEN not set!")
                  return
              
              results = {}
              
              for swimmer_id, swimmer_info in SWIMMERS.items():
                  data = run_apify_scraper_for_swimmer(swimmer_id, swimmer_info)
                  if data:
                      results[swimmer_info["name"]] = data
                  time.sleep(5)  # Rate limit protection
              
              # Save results
              output = {
                  "scraped_at": datetime.now(timezone.utc).isoformat(),
                  "version": "APIFY_PRO_V1",
                  "school": "Satellite Beach High School",
                  "team": "Swim Melbourne (MELB-FL)",
                  "method": "Apify Web Scraper with RESIDENTIAL proxies",
                  "swimmer_count": len(results),
                  "swimmers": results
              }
              
              with open("satellite_beach_swimmers.json", "w") as f:
                  json.dump(output, f, indent=2)
              
              # Summary
              print("\n" + "="*60)
              print("FINAL SUMMARY")
              print("="*60)
              
              success_count = 0
              for name, data in results.items():
                  pbs = data.get("personal_bests", {})
                  if pbs:
                      success_count += 1
                      print(f"\nâœ… {name}: {len(pbs)} events")
                      for e in ['50 Free', '100 Free', '200 Free', '100 Fly', '100 Back', '100 Breast']:
                          if e in pbs:
                              print(f"   {e}: {pbs[e]['time']}")
                  else:
                      print(f"\nâŒ {name}: NO DATA")
              
              print(f"\nSuccess Rate: {success_count}/{len(SWIMMERS)} ({100*success_count/len(SWIMMERS):.0f}%)")
              
              # Save to Supabase
              if SUPABASE_KEY:
                  payload = {
                      "category": "michael_swim",
                      "subcategory": "satellite_beach_team",
                      "title": f"Satellite Beach Team Scrape {datetime.now().strftime('%Y-%m-%d')}",
                      "insight": json.dumps(output),
                      "importance": "high"
                  }
                  
                  resp = requests.post(
                      f"{SUPABASE_URL}/rest/v1/insights",
                      headers={
                          "apikey": SUPABASE_KEY,
                          "Authorization": f"Bearer {SUPABASE_KEY}",
                          "Content-Type": "application/json"
                      },
                      json=payload
                  )
                  print(f"\n{'âœ…' if resp.status_code in [200,201] else 'âŒ'} Supabase: {resp.status_code}")

          if __name__ == "__main__":
              main()
          PYTHON_EOF
      
      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: satellite-beach-apify-${{ github.run_number }}
          path: satellite_beach_swimmers.json
      
      - name: Commit to Repo
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          mkdir -p michael_d1_agents_v3/data
          cp satellite_beach_swimmers.json michael_d1_agents_v3/data/
          git add michael_d1_agents_v3/data/satellite_beach_swimmers.json
          git diff --staged --quiet || git commit -m "data: Satellite Beach APIFY PRO scrape $(date +'%Y-%m-%d %H:%M')"
          git push || echo "Nothing to push"
      
      - name: Display Results
        run: cat satellite_beach_swimmers.json
