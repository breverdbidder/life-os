name: Satellite Beach SwimCloud Scraper

on:
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Playwright
        run: |
          pip install playwright beautifulsoup4
          playwright install chromium
          playwright install-deps chromium
      
      - name: Deep Scrape All Satellite Beach Swimmers
        run: |
          python3 << 'PYTHON_EOF'
          import asyncio
          import json
          import re
          from datetime import datetime
          from playwright.async_api import async_playwright

          SWIMMERS = {
              "3250085": {"name": "Michael Shapira", "class": 2027},
              "2928537": {"name": "Bastian Soto", "class": 2027},
              "1518102": {"name": "Gavin Domboru", "class": 2027},
              "1754554": {"name": "Max Morgan", "class": 2026},
              "2662363": {"name": "Sawyer Davis", "class": 2026, "note": "Ohio State Commit, 2A State Champion"}
          }

          def parse_time(time_str):
              """Convert time string to seconds."""
              try:
                  time_str = time_str.strip()
                  if ':' in time_str:
                      parts = time_str.split(':')
                      return round(float(parts[0]) * 60 + float(parts[1]), 2)
                  return round(float(time_str), 2)
              except:
                  return None

          async def scrape_swimmer_deep(page, swimmer_id, swimmer_info):
              """Deep scrape - get ALL times from times page."""
              print(f"\n{'='*70}")
              print(f"DEEP SCRAPE: {swimmer_info['name']} (ID: {swimmer_id})")
              print(f"{'='*70}")
              
              data = {
                  "swimmer_id": swimmer_id,
                  "name": swimmer_info["name"],
                  "class_of": swimmer_info.get("class"),
                  "school": "Satellite Sr High School",
                  "team": "Swim Melbourne (MELB-FL)",
                  "note": swimmer_info.get("note", ""),
                  "scraped_at": datetime.now().isoformat(),
                  "profile": {},
                  "personal_bests": {},
                  "all_times": {}
              }
              
              try:
                  # First get profile info
                  url = f"https://www.swimcloud.com/swimmer/{swimmer_id}/"
                  await page.goto(url, wait_until="networkidle", timeout=60000)
                  await page.wait_for_timeout(3000)
                  
                  body_text = await page.locator("body").text_content()
                  
                  # Power Index
                  pi_match = re.search(r'Power\s*index\s*(\d+\.?\d*)', body_text, re.I)
                  if pi_match:
                      data["profile"]["power_index"] = float(pi_match.group(1))
                  
                  # Florida Rank
                  fl_rank = re.search(r'Florida\s*rank\s*(\d+)', body_text, re.I)
                  if fl_rank:
                      data["profile"]["florida_rank"] = int(fl_rank.group(1))
                  
                  print(f"  Profile: PI={data['profile'].get('power_index', 'N/A')}, FL Rank={data['profile'].get('florida_rank', 'N/A')}")
                  
                  # Now go to TIMES page for full history
                  times_url = f"https://www.swimcloud.com/swimmer/{swimmer_id}/times/"
                  print(f"  Loading times page: {times_url}")
                  await page.goto(times_url, wait_until="networkidle", timeout=60000)
                  await page.wait_for_timeout(5000)
                  
                  # Click "SCY" tab if exists to ensure short course yards
                  try:
                      scy_tab = page.locator("text=SCY").first
                      if await scy_tab.is_visible():
                          await scy_tab.click()
                          await page.wait_for_timeout(2000)
                          print(f"  Clicked SCY tab")
                  except:
                      pass
                  
                  # Get the full HTML
                  html = await page.content()
                  
                  # Define events to search for
                  events = [
                      ("50 Free", r'50\s*(?:Y(?:ard)?|SCY)?\s*Free'),
                      ("100 Free", r'100\s*(?:Y(?:ard)?|SCY)?\s*Free'),
                      ("200 Free", r'200\s*(?:Y(?:ard)?|SCY)?\s*Free'),
                      ("500 Free", r'500\s*(?:Y(?:ard)?|SCY)?\s*Free'),
                      ("50 Fly", r'50\s*(?:Y(?:ard)?|SCY)?\s*(?:Fly|Butterfly)'),
                      ("100 Fly", r'100\s*(?:Y(?:ard)?|SCY)?\s*(?:Fly|Butterfly)'),
                      ("50 Back", r'50\s*(?:Y(?:ard)?|SCY)?\s*Back'),
                      ("100 Back", r'100\s*(?:Y(?:ard)?|SCY)?\s*Back'),
                      ("200 Back", r'200\s*(?:Y(?:ard)?|SCY)?\s*Back'),
                      ("50 Breast", r'50\s*(?:Y(?:ard)?|SCY)?\s*Breast'),
                      ("100 Breast", r'100\s*(?:Y(?:ard)?|SCY)?\s*Breast'),
                      ("200 Breast", r'200\s*(?:Y(?:ard)?|SCY)?\s*Breast'),
                      ("200 IM", r'200\s*(?:Y(?:ard)?|SCY)?\s*(?:IM|Individual\s*Medley)'),
                      ("400 IM", r'400\s*(?:Y(?:ard)?|SCY)?\s*(?:IM|Individual\s*Medley)'),
                  ]
                  
                  # Try to find times table rows
                  # SwimCloud format: Event name followed by time
                  for event_name, event_pattern in events:
                      # Look for the event and capture the time that follows
                      # Pattern: event name ... time (like 21.86 or 1:48.50)
                      full_pattern = event_pattern + r'[\s\S]{0,200}?(\d{1,2}:\d{2}\.\d{2}|\d{2}\.\d{2})'
                      matches = re.findall(full_pattern, html, re.I)
                      
                      if matches:
                          # Get all times found, take the fastest (lowest)
                          times_found = []
                          for match in matches:
                              seconds = parse_time(match)
                              if seconds and seconds > 15 and seconds < 600:  # Reasonable range
                                  times_found.append((match, seconds))
                          
                          if times_found:
                              # Sort by seconds (fastest first)
                              times_found.sort(key=lambda x: x[1])
                              best_time, best_seconds = times_found[0]
                              
                              data["personal_bests"][event_name] = {
                                  "time": best_time,
                                  "seconds": best_seconds
                              }
                              data["all_times"][event_name] = [{"time": t, "seconds": s} for t, s in times_found[:5]]
                  
                  # Also try clicking on each event to get detailed times
                  # Look for event links
                  event_links = await page.locator("a[href*='/times/']").all()
                  print(f"  Found {len(event_links)} event links")
                  
                  for link in event_links[:10]:  # Limit to 10 events
                      try:
                          link_text = await link.text_content()
                          link_href = await link.get_attribute("href")
                          
                          if not link_href or '/times/' not in link_href:
                              continue
                          
                          # Check if it's an event we care about
                          event_match = None
                          for event_name, event_pattern in events:
                              if re.search(event_pattern, link_text, re.I):
                                  event_match = event_name
                                  break
                          
                          if event_match and event_match not in data["personal_bests"]:
                              # Click to get times
                              await link.click()
                              await page.wait_for_timeout(2000)
                              
                              event_html = await page.content()
                              time_matches = re.findall(r'(\d{1,2}:\d{2}\.\d{2}|\d{2}\.\d{2})', event_html)
                              
                              valid_times = []
                              for t in time_matches:
                                  s = parse_time(t)
                                  if s and s > 15 and s < 600:
                                      valid_times.append((t, s))
                              
                              if valid_times:
                                  valid_times.sort(key=lambda x: x[1])
                                  best_time, best_seconds = valid_times[0]
                                  data["personal_bests"][event_match] = {
                                      "time": best_time,
                                      "seconds": best_seconds
                                  }
                              
                              # Go back
                              await page.go_back()
                              await page.wait_for_timeout(1000)
                      except Exception as e:
                          continue
                  
                  # Print results
                  print(f"\n  PERSONAL BESTS FOUND ({len(data['personal_bests'])} events):")
                  for event in ['50 Free', '100 Free', '200 Free', '50 Fly', '100 Fly', '100 Back', '100 Breast', '200 IM']:
                      if event in data["personal_bests"]:
                          print(f"    {event}: {data['personal_bests'][event]['time']}")
                  
                  return data
                  
              except Exception as e:
                  print(f"  ERROR: {e}")
                  return {
                      "swimmer_id": swimmer_id,
                      "name": swimmer_info["name"],
                      "error": str(e)
                  }

          async def main():
              print("ðŸŠ SATELLITE BEACH DEEP SCRAPER V3")
              print("="*70)
              print(f"Scraping {len(SWIMMERS)} swimmers with FULL time history")
              print("="*70)
              
              results = {}
              
              async with async_playwright() as p:
                  browser = await p.chromium.launch(
                      headless=True,
                      args=['--no-sandbox', '--disable-setuid-sandbox']
                  )
                  context = await browser.new_context(
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                      viewport={"width": 1920, "height": 1080}
                  )
                  page = await context.new_page()
                  
                  for swimmer_id, swimmer_info in SWIMMERS.items():
                      data = await scrape_swimmer_deep(page, swimmer_id, swimmer_info)
                      results[swimmer_info["name"]] = data
                      await asyncio.sleep(2)
                  
                  await browser.close()
              
              output = {
                  "scraped_at": datetime.now().isoformat(),
                  "version": "V3_DEEP",
                  "school": "Satellite Beach High School",
                  "team": "Swim Melbourne (MELB-FL)",
                  "swimmer_count": len(SWIMMERS),
                  "swimmers": results
              }
              
              with open("satellite_beach_swimmers.json", "w") as f:
                  json.dump(output, f, indent=2)
              
              # Final Summary
              print("\n" + "="*70)
              print("FINAL RESULTS - SATELLITE BEACH HIGH SCHOOL")
              print("="*70)
              
              for name, data in results.items():
                  pbs = data.get("personal_bests", {})
                  pi = data.get('profile', {}).get('power_index', 'N/A')
                  fl = data.get('profile', {}).get('florida_rank', 'N/A')
                  note = data.get('note', '')
                  
                  print(f"\n{name} (Class {data.get('class_of', '?')})")
                  if note:
                      print(f"  â­ {note}")
                  print(f"  Power Index: {pi} | FL Rank: {fl}")
                  for event in ['50 Free', '100 Free', '200 Free', '50 Fly', '100 Fly', '100 Back', '100 Breast']:
                      if event in pbs:
                          print(f"  {event}: {pbs[event]['time']}")
              
              # Comparison table
              print("\n" + "="*70)
              print("100 FREE COMPARISON")
              print("="*70)
              free_100 = []
              for name, data in results.items():
                  if "100 Free" in data.get("personal_bests", {}):
                      free_100.append((name, data["personal_bests"]["100 Free"]["time"], data["personal_bests"]["100 Free"]["seconds"]))
              
              free_100.sort(key=lambda x: x[2])
              for i, (name, time, secs) in enumerate(free_100, 1):
                  print(f"  {i}. {name}: {time}")
              
              print(f"\nðŸ’¾ Saved to satellite_beach_swimmers.json")
              return output

          if __name__ == "__main__":
              asyncio.run(main())
          PYTHON_EOF
      
      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: satellite-beach-deep-${{ github.run_number }}
          path: satellite_beach_swimmers.json
      
      - name: Commit to Repo
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          mkdir -p michael_d1_agents_v3/data
          cp satellite_beach_swimmers.json michael_d1_agents_v3/data/
          git add michael_d1_agents_v3/data/satellite_beach_swimmers.json
          git diff --staged --quiet || git commit -m "data: Satellite Beach DEEP scrape V3 $(date +'%Y-%m-%d %H:%M')"
          git push || echo "Nothing to push"
      
      - name: Display Results
        run: cat satellite_beach_swimmers.json
