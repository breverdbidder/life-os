name: SwimCloud PB Scraper

on:
  workflow_dispatch:
    inputs:
      swimmer_id:
        description: 'SwimCloud Swimmer ID'
        required: true
        default: '3250085'
      swimmer_name:
        description: 'Swimmer Name'
        required: true
        default: 'Michael Shapira'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install playwright beautifulsoup4 requests
          playwright install chromium
      
      - name: Scrape SwimCloud
        id: scrape
        run: |
          python3 << 'SCRAPE_EOF'
          import asyncio
          import json
          import re
          import os
          from datetime import datetime
          from bs4 import BeautifulSoup
          from playwright.async_api import async_playwright

          SWIMMER_ID = "${{ github.event.inputs.swimmer_id }}"
          SWIMMER_NAME = "${{ github.event.inputs.swimmer_name }}"

          async def scrape_swimcloud():
              print(f"ðŸŠ Scraping SwimCloud for {SWIMMER_NAME} (ID: {SWIMMER_ID})")
              
              async with async_playwright() as p:
                  browser = await p.chromium.launch(headless=True)
                  context = await browser.new_context(
                      user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                  )
                  page = await context.new_page()
                  
                  # Navigate to times page
                  url = f"https://www.swimcloud.com/swimmer/{SWIMMER_ID}/times/"
                  print(f"Navigating to: {url}")
                  
                  await page.goto(url, wait_until='networkidle', timeout=60000)
                  await asyncio.sleep(5)  # Wait for full render
                  
                  # Get page content
                  content = await page.content()
                  print(f"Page loaded: {len(content)} bytes")
                  
                  # Parse with BeautifulSoup
                  soup = BeautifulSoup(content, 'html.parser')
                  
                  # Extract swimmer name
                  name_elem = soup.select_one('h1, .swimmer-name, [class*="name"]')
                  print(f"Name element: {name_elem.text if name_elem else 'Not found'}")
                  
                  # Find all time tables
                  tables = soup.find_all('table')
                  print(f"Found {len(tables)} tables")
                  
                  personal_bests = []
                  
                  # Look for PB times in various formats
                  # SwimCloud typically shows times in tables with event, time, meet, date
                  for table in tables:
                      rows = table.find_all('tr')
                      for row in rows:
                          cells = row.find_all(['td', 'th'])
                          if len(cells) >= 2:
                              text = ' | '.join([c.get_text(strip=True) for c in cells])
                              # Look for time patterns
                              if re.search(r'\d{1,2}:\d{2}\.\d{2}|\d{2}\.\d{2}', text):
                                  personal_bests.append(text)
                  
                  # Also try to find times in divs/spans
                  time_elements = soup.find_all(string=re.compile(r'^\d{1,2}:\d{2}\.\d{2}$|^\d{2}\.\d{2}$'))
                  for elem in time_elements:
                      parent = elem.parent
                      if parent:
                          # Get context around the time
                          context_text = parent.get_text(strip=True)
                          if context_text not in personal_bests:
                              personal_bests.append(context_text)
                  
                  # Get full page text for parsing
                  page_text = soup.get_text(separator='\n', strip=True)
                  
                  # Save results
                  results = {
                      "swimmer_id": SWIMMER_ID,
                      "swimmer_name": SWIMMER_NAME,
                      "scraped_at": datetime.now().isoformat(),
                      "url": url,
                      "personal_bests_raw": personal_bests[:50],  # First 50
                      "page_text_sample": page_text[:5000]
                  }
                  
                  with open('swimcloud_results.json', 'w') as f:
                      json.dump(results, f, indent=2)
                  
                  # Also save full HTML for debugging
                  with open('swimcloud_full.html', 'w') as f:
                      f.write(content)
                  
                  print(f"\nâœ… Found {len(personal_bests)} potential time entries")
                  print(f"Sample entries: {personal_bests[:5]}")
                  
                  await browser.close()
                  return results

          results = asyncio.run(scrape_swimcloud())
          print(json.dumps(results, indent=2))
          SCRAPE_EOF
      
      - name: Parse and Structure Data
        run: |
          python3 << 'PARSE_EOF'
          import json
          import re
          from datetime import datetime

          # Load raw results
          with open('swimcloud_results.json', 'r') as f:
              raw = json.load(f)

          print("ðŸ” Parsing SwimCloud data...")
          print(f"Raw entries: {len(raw.get('personal_bests_raw', []))}")

          # Parse the page text to extract structured PB data
          page_text = raw.get('page_text_sample', '')
          
          # Common event names
          events = ['50 Free', '100 Free', '200 Free', '500 Free', '1000 Free', '1650 Free',
                    '50 Back', '100 Back', '200 Back',
                    '50 Breast', '100 Breast', '200 Breast', 
                    '50 Fly', '100 Fly', '200 Fly',
                    '100 IM', '200 IM', '400 IM']

          # Time pattern
          time_pattern = r'(\d{1,2}:\d{2}\.\d{2}|\d{2}\.\d{2})'
          
          # Date patterns
          date_patterns = [
              r'(\w{3,9}\s+\d{1,2},?\s+\d{4})',  # Month DD, YYYY
              r'(\d{1,2}/\d{1,2}/\d{2,4})',       # MM/DD/YYYY
          ]

          structured_pbs = []
          
          # Try to match events with times
          for event in events:
              # Find event mentions in text
              event_pattern = re.compile(rf'{event}[^\n]*?({time_pattern})', re.IGNORECASE)
              matches = event_pattern.findall(page_text)
              if matches:
                  for match in matches:
                      time = match[0] if isinstance(match, tuple) else match
                      structured_pbs.append({
                          "event": event,
                          "time": time,
                          "course": "SCY",  # Default
                          "source": "swimcloud"
                      })

          # Save structured data
          output = {
              "swimmer_id": raw['swimmer_id'],
              "swimmer_name": raw['swimmer_name'],
              "scraped_at": raw['scraped_at'],
              "personal_bests": structured_pbs
          }

          with open('michael_pbs_structured.json', 'w') as f:
              json.dump(output, f, indent=2)

          print(f"\nâœ… Structured {len(structured_pbs)} personal bests")
          print(json.dumps(output, indent=2))
          PARSE_EOF
      
      - name: Upload to Supabase
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          python3 << 'SUPABASE_EOF'
          import os
          import json
          import requests

          SUPABASE_URL = os.environ.get('SUPABASE_URL')
          SUPABASE_KEY = os.environ.get('SUPABASE_KEY')

          if not SUPABASE_URL or not SUPABASE_KEY:
              print("âš ï¸ Supabase credentials not available")
              exit(0)

          # Load structured data
          with open('michael_pbs_structured.json', 'r') as f:
              data = json.load(f)

          headers = {
              "apikey": SUPABASE_KEY,
              "Authorization": f"Bearer {SUPABASE_KEY}",
              "Content-Type": "application/json"
          }

          # Insert into personal_best_times table
          for pb in data.get('personal_bests', []):
              record = {
                  "swimmer_name": data['swimmer_name'],
                  "event": pb['event'],
                  "time_seconds": float(pb['time'].replace(':', '')) if ':' not in pb['time'] else 
                                  float(pb['time'].split(':')[0]) * 60 + float(pb['time'].split(':')[1]),
                  "source": "swimcloud",
                  "verified": True
              }
              
              resp = requests.post(
                  f"{SUPABASE_URL}/rest/v1/personal_best_times",
                  headers=headers,
                  json=record
              )
              print(f"{pb['event']}: {resp.status_code}")

          print("âœ… Uploaded to Supabase")
          SUPABASE_EOF
      
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: swimcloud-data
          path: |
            swimcloud_results.json
            michael_pbs_structured.json
            swimcloud_full.html
