name: SwimCloud Scraper - Michael Shapira

on:
  workflow_dispatch:
    inputs:
      swimmer_id:
        description: 'SwimCloud Swimmer ID'
        required: true
        default: '3250085'
      swimmer_name:
        description: 'Swimmer Name'
        required: true
        default: 'Michael Shapira'

jobs:
  scrape-swimcloud:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: pip install requests
          
      - name: Scrape SwimCloud with Apify
        env:
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          SWIMMER_ID: ${{ github.event.inputs.swimmer_id || '3250085' }}
          SWIMMER_NAME: ${{ github.event.inputs.swimmer_name || 'Michael Shapira' }}
        run: |
          python3 << 'SCRAPER_EOF'
          import os
          import json
          import time
          import requests
          from datetime import datetime, timezone
          
          APIFY_TOKEN = os.environ.get('APIFY_API_TOKEN')
          SUPABASE_URL = os.environ.get('SUPABASE_URL', 'https://mocerqjnksmhcjzxrewo.supabase.co')
          SUPABASE_KEY = os.environ.get('SUPABASE_KEY')
          SWIMMER_ID = os.environ.get('SWIMMER_ID', '3250085')
          SWIMMER_NAME = os.environ.get('SWIMMER_NAME', 'Michael Shapira')
          
          print(f"üèä SwimCloud Scraper")
          print(f"   Swimmer: {SWIMMER_NAME} (ID: {SWIMMER_ID})")
          print(f"   Token: {'SET' if APIFY_TOKEN else 'NOT SET'}")
          
          if not APIFY_TOKEN:
              print("‚ùå APIFY_API_TOKEN not set!")
              exit(1)
          
          # URLs to scrape
          urls = [
              f"https://www.swimcloud.com/swimmer/{SWIMMER_ID}/",
              f"https://www.swimcloud.com/swimmer/{SWIMMER_ID}/times/"
          ]
          
          # Apify Web Scraper config
          scraper_input = {
              "startUrls": [{"url": url} for url in urls],
              "pageFunction": """
          async function pageFunction(context) {
              const { request, page, log } = context;
              log.info('Processing: ' + request.url);
              
              await page.waitForTimeout(5000);
              
              const data = await page.evaluate(() => {
                  const result = {
                      url: window.location.href,
                      name: '',
                      team: '',
                      times: {},
                      raw_times: [],
                      text_sample: ''
                  };
                  
                  // Get swimmer name
                  const h1 = document.querySelector('h1');
                  if (h1) result.name = h1.textContent.trim();
                  
                  // Get team
                  const teamEl = document.querySelector('a[href*="/team/"]');
                  if (teamEl) result.team = teamEl.textContent.trim();
                  
                  // Get page text
                  const bodyText = document.body.innerText;
                  result.text_sample = bodyText.substring(0, 3000);
                  
                  // Find all table rows with times
                  document.querySelectorAll('tr').forEach(row => {
                      const text = row.textContent;
                      const timeMatch = text.match(/(\\d{1,2}:\\d{2}\\.\\d{2}|\\d{2}\\.\\d{2})/);
                      if (timeMatch) {
                          result.raw_times.push({
                              row: text.trim().substring(0, 200),
                              time: timeMatch[1]
                          });
                      }
                  });
                  
                  // Parse specific events
                  const eventPatterns = {
                      '50 Free': /50\\s*(?:Y(?:ard)?)?\\s*Free[^\\d]*(\\d{2}\\.\\d{2})/gi,
                      '100 Free': /100\\s*(?:Y(?:ard)?)?\\s*Free[^\\d]*(\\d{2}\\.\\d{2}|\\d:\\d{2}\\.\\d{2})/gi,
                      '200 Free': /200\\s*(?:Y(?:ard)?)?\\s*Free[^\\d]*(\\d:\\d{2}\\.\\d{2})/gi,
                      '100 Fly': /100\\s*(?:Y(?:ard)?)?\\s*(?:Fly|Butterfly)[^\\d]*(\\d{2}\\.\\d{2}|\\d:\\d{2}\\.\\d{2})/gi,
                      '100 Back': /100\\s*(?:Y(?:ard)?)?\\s*Back[^\\d]*(\\d{2}\\.\\d{2}|\\d:\\d{2}\\.\\d{2})/gi
                  };
                  
                  for (const [event, pattern] of Object.entries(eventPatterns)) {
                      const match = pattern.exec(bodyText);
                      if (match) {
                          result.times[event] = match[1];
                      }
                  }
                  
                  return result;
              });
              
              return data;
          }
              """,
              "proxyConfiguration": {"useApifyProxy": True},
              "maxConcurrency": 1,
              "maxRequestsPerCrawl": 5
          }
          
          print("\nüì° Starting Apify Web Scraper...")
          
          response = requests.post(
              "https://api.apify.com/v2/acts/apify~web-scraper/runs",
              headers={
                  "Authorization": f"Bearer {APIFY_TOKEN}",
                  "Content-Type": "application/json"
              },
              json=scraper_input,
              timeout=60
          )
          
          if response.status_code != 201:
              print(f"‚ùå Failed to start: {response.status_code}")
              print(response.text[:500])
              exit(1)
          
          run_data = response.json()
          run_id = run_data["data"]["id"]
          print(f"‚úÖ Run started: {run_id}")
          
          # Wait for completion
          for i in range(24):
              time.sleep(10)
              
              status_resp = requests.get(
                  f"https://api.apify.com/v2/actor-runs/{run_id}",
                  headers={"Authorization": f"Bearer {APIFY_TOKEN}"},
                  timeout=30
              )
              
              status = status_resp.json()["data"]["status"]
              print(f"   [{i*10}s] Status: {status}")
              
              if status in ["SUCCEEDED", "FAILED", "ABORTED", "TIMED-OUT"]:
                  break
          
          if status != "SUCCEEDED":
              print(f"‚ùå Run failed: {status}")
              exit(1)
          
          # Get results
          dataset_id = status_resp.json()["data"]["defaultDatasetId"]
          results = requests.get(
              f"https://api.apify.com/v2/datasets/{dataset_id}/items",
              headers={"Authorization": f"Bearer {APIFY_TOKEN}"},
              timeout=30
          ).json()
          
          print(f"\n{'='*50}")
          print(f"üìä RESULTS FOR {SWIMMER_NAME}")
          print(f"{'='*50}")
          
          # Process results
          all_times = {}
          raw_data = []
          
          for item in results:
              if 'times' in item:
                  all_times.update(item.get('times', {}))
              if 'raw_times' in item:
                  raw_data.extend(item.get('raw_times', []))
              if 'text_sample' in item:
                  print(f"\n--- Page Text Sample ---")
                  print(item['text_sample'][:1000])
          
          if all_times:
              print(f"\nüèä PERSONAL BEST TIMES:")
              for event, t in sorted(all_times.items()):
                  print(f"   {event}: {t}")
          else:
              print("\n‚ö†Ô∏è No times extracted via patterns")
              print(f"Raw time data found: {len(raw_data)} rows")
              for row in raw_data[:10]:
                  print(f"   {row}")
          
          # Save results
          output = {
              "swimmer_name": SWIMMER_NAME,
              "swimmer_id": SWIMMER_ID,
              "times": all_times,
              "raw_times": raw_data[:20],
              "source": "swimcloud.com",
              "scraped_at": datetime.now(timezone.utc).isoformat()
          }
          
          with open(f"swimcloud_{SWIMMER_ID}.json", 'w') as f:
              json.dump(output, f, indent=2)
          
          print(f"\nüíæ Saved to swimcloud_{SWIMMER_ID}.json")
          
          # Save to Supabase
          if SUPABASE_KEY:
              payload = {
                  "category": "michael_swim",
                  "subcategory": "swimcloud_pbs",
                  "title": f"SwimCloud Times - {SWIMMER_NAME}",
                  "insight": json.dumps(output),
                  "importance": "high",
                  "created_at": datetime.now(timezone.utc).isoformat()
              }
              
              resp = requests.post(
                  f"{SUPABASE_URL}/rest/v1/insights",
                  headers={
                      "apikey": SUPABASE_KEY,
                      "Authorization": f"Bearer {SUPABASE_KEY}",
                      "Content-Type": "application/json"
                  },
                  json=payload,
                  timeout=30
              )
              
              if resp.status_code in [200, 201]:
                  print("‚úÖ Saved to Supabase")
              else:
                  print(f"‚ö†Ô∏è Supabase: {resp.status_code}")
          
          print("\n‚úÖ SCRAPING COMPLETE")
          SCRAPER_EOF
        
      - name: Upload Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: swimcloud-results-${{ github.run_id }}
          path: swimcloud_*.json
          retention-days: 7
