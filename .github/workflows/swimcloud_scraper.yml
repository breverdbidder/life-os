name: SwimCloud Scraper - Michael Shapira

on:
  workflow_dispatch:
    inputs:
      swimmer_id:
        description: 'SwimCloud Swimmer ID'
        required: true
        default: '3250085'
      swimmer_name:
        description: 'Swimmer Name'
        required: true
        default: 'Michael Shapira'

jobs:
  scrape-swimcloud:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: pip install requests
          
      - name: Scrape SwimCloud with Apify
        env:
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          SWIMMER_ID: ${{ github.event.inputs.swimmer_id || '3250085' }}
          SWIMMER_NAME: ${{ github.event.inputs.swimmer_name || 'Michael Shapira' }}
        run: |
          python3 << 'SCRAPER_EOF'
          import os
          import json
          import time
          import requests
          from datetime import datetime, timezone
          
          APIFY_TOKEN = os.environ.get('APIFY_API_TOKEN')
          SUPABASE_URL = os.environ.get('SUPABASE_URL', 'https://mocerqjnksmhcjzxrewo.supabase.co')
          SUPABASE_KEY = os.environ.get('SUPABASE_KEY')
          SWIMMER_ID = os.environ.get('SWIMMER_ID', '3250085')
          SWIMMER_NAME = os.environ.get('SWIMMER_NAME', 'Michael Shapira')
          
          print(f"üèä SwimCloud Scraper V2 (Fixed)")
          print(f"   Swimmer: {SWIMMER_NAME} (ID: {SWIMMER_ID})")
          
          if not APIFY_TOKEN:
              print("‚ùå APIFY_API_TOKEN not set!")
              exit(1)
          
          urls = [
              f"https://www.swimcloud.com/swimmer/{SWIMMER_ID}/",
              f"https://www.swimcloud.com/swimmer/{SWIMMER_ID}/times/"
          ]
          
          # Fixed pageFunction - no waitForTimeout
          scraper_input = {
              "startUrls": [{"url": url} for url in urls],
              "pageFunction": """
          async function pageFunction(context) {
              const { request, page, log } = context;
              log.info('Processing: ' + request.url);
              
              // Wait for page to load using waitForSelector instead
              await page.waitForSelector('body', { timeout: 30000 });
              
              // Additional wait using page.waitForFunction
              await page.waitForFunction(() => document.readyState === 'complete', { timeout: 30000 });
              
              const data = await page.evaluate(() => {
                  const result = {
                      url: window.location.href,
                      title: document.title,
                      name: '',
                      team: '',
                      times: {},
                      raw_times: [],
                      text_sample: ''
                  };
                  
                  // Get swimmer name from h1 or title
                  const h1 = document.querySelector('h1');
                  if (h1) result.name = h1.textContent.trim();
                  
                  // Get team
                  const teamEl = document.querySelector('a[href*="/team/"]');
                  if (teamEl) result.team = teamEl.textContent.trim();
                  
                  // Get all body text
                  const bodyText = document.body.innerText || '';
                  result.text_sample = bodyText.substring(0, 5000);
                  
                  // Find times in tables
                  document.querySelectorAll('tr, .time-entry, [class*="time"]').forEach(el => {
                      const text = el.textContent || '';
                      const timeMatch = text.match(/(\\d{1,2}:\\d{2}\\.\\d{2}|\\d{2}\\.\\d{2})/);
                      if (timeMatch) {
                          result.raw_times.push({
                              text: text.trim().substring(0, 300),
                              time: timeMatch[1]
                          });
                      }
                  });
                  
                  // Parse specific events from body text
                  const patterns = {
                      '50 Free': /50\\s*(?:Yard|Y)?\\s*Free[^\\d]*(\\d{2}\\.\\d{2})/i,
                      '100 Free': /100\\s*(?:Yard|Y)?\\s*Free[^\\d]*(\\d{2}\\.\\d{2}|\\d:\\d{2}\\.\\d{2})/i,
                      '200 Free': /200\\s*(?:Yard|Y)?\\s*Free[^\\d]*(\\d:\\d{2}\\.\\d{2})/i,
                      '100 Fly': /100\\s*(?:Yard|Y)?\\s*(?:Fly|Butterfly)[^\\d]*(\\d{2}\\.\\d{2}|\\d:\\d{2}\\.\\d{2})/i,
                      '100 Back': /100\\s*(?:Yard|Y)?\\s*Back[^\\d]*(\\d{2}\\.\\d{2}|\\d:\\d{2}\\.\\d{2})/i
                  };
                  
                  for (const [event, pattern] of Object.entries(patterns)) {
                      const match = bodyText.match(pattern);
                      if (match) {
                          result.times[event] = match[1];
                      }
                  }
                  
                  return result;
              });
              
              return data;
          }
              """,
              "proxyConfiguration": {"useApifyProxy": True},
              "maxConcurrency": 1,
              "maxRequestsPerCrawl": 5,
              "maxRequestRetries": 3
          }
          
          print("\nüì° Starting Apify Web Scraper...")
          
          response = requests.post(
              "https://api.apify.com/v2/acts/apify~web-scraper/runs",
              headers={
                  "Authorization": f"Bearer {APIFY_TOKEN}",
                  "Content-Type": "application/json"
              },
              json=scraper_input,
              timeout=60
          )
          
          if response.status_code != 201:
              print(f"‚ùå Failed to start: {response.status_code}")
              print(response.text[:500])
              exit(1)
          
          run_data = response.json()
          run_id = run_data["data"]["id"]
          print(f"‚úÖ Run started: {run_id}")
          
          # Wait for completion (max 4 min)
          for i in range(24):
              time.sleep(10)
              
              status_resp = requests.get(
                  f"https://api.apify.com/v2/actor-runs/{run_id}",
                  headers={"Authorization": f"Bearer {APIFY_TOKEN}"},
                  timeout=30
              )
              
              status = status_resp.json()["data"]["status"]
              print(f"   [{i*10}s] Status: {status}")
              
              if status in ["SUCCEEDED", "FAILED", "ABORTED", "TIMED-OUT"]:
                  break
          
          # Get results even if failed
          dataset_id = status_resp.json()["data"]["defaultDatasetId"]
          results = requests.get(
              f"https://api.apify.com/v2/datasets/{dataset_id}/items",
              headers={"Authorization": f"Bearer {APIFY_TOKEN}"},
              timeout=30
          ).json()
          
          print(f"\n{'='*50}")
          print(f"üìä RESULTS FOR {SWIMMER_NAME}")
          print(f"{'='*50}")
          
          all_times = {}
          raw_data = []
          text_samples = []
          
          for item in results:
              print(f"\n--- Page: {item.get('url', 'unknown')} ---")
              print(f"Title: {item.get('title', 'N/A')}")
              print(f"Name found: {item.get('name', 'N/A')}")
              print(f"Team: {item.get('team', 'N/A')}")
              
              if 'times' in item:
                  all_times.update(item.get('times', {}))
              if 'raw_times' in item:
                  raw_data.extend(item.get('raw_times', []))
              if 'text_sample' in item:
                  text_samples.append(item['text_sample'])
                  print(f"\nText sample (first 1000 chars):")
                  print(item['text_sample'][:1000])
          
          if all_times:
              print(f"\nüèä PERSONAL BEST TIMES:")
              for event, t in sorted(all_times.items()):
                  print(f"   {event}: {t}")
          else:
              print(f"\n‚ö†Ô∏è No times extracted via patterns")
              print(f"Raw time entries found: {len(raw_data)}")
              for row in raw_data[:10]:
                  print(f"   {row}")
          
          # Save all data
          output = {
              "swimmer_name": SWIMMER_NAME,
              "swimmer_id": SWIMMER_ID,
              "times": all_times,
              "raw_times": raw_data[:30],
              "text_samples": text_samples,
              "source": "swimcloud.com",
              "apify_run_id": run_id,
              "scraped_at": datetime.now(timezone.utc).isoformat()
          }
          
          with open(f"swimcloud_{SWIMMER_ID}.json", 'w') as f:
              json.dump(output, f, indent=2)
          
          print(f"\nüíæ Saved to swimcloud_{SWIMMER_ID}.json")
          
          # Save to Supabase
          if SUPABASE_KEY and (all_times or raw_data):
              payload = {
                  "category": "michael_swim",
                  "subcategory": "swimcloud_pbs",
                  "title": f"SwimCloud Times - {SWIMMER_NAME}",
                  "insight": json.dumps({
                      "times": all_times,
                      "raw_times": raw_data[:10],
                      "scraped_at": output["scraped_at"]
                  }),
                  "importance": "high",
                  "created_at": datetime.now(timezone.utc).isoformat()
              }
              
              resp = requests.post(
                  f"{SUPABASE_URL}/rest/v1/insights",
                  headers={
                      "apikey": SUPABASE_KEY,
                      "Authorization": f"Bearer {SUPABASE_KEY}",
                      "Content-Type": "application/json"
                  },
                  json=payload,
                  timeout=30
              )
              print(f"Supabase: {resp.status_code}")
          
          print("\n‚úÖ SCRAPING COMPLETE")
          SCRAPER_EOF
        
      - name: Upload Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: swimcloud-results-${{ github.run_id }}
          path: swimcloud_*.json
          retention-days: 7
