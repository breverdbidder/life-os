name: Website Scraper V2 - Multi-Provider

on:
  workflow_dispatch:
    inputs:
      url:
        description: 'Website URL to scrape'
        required: true
        type: string
      project_name:
        description: 'Output project name'
        required: true
        default: 'scraped-site'
        type: string
      scrape_method:
        description: 'Scraping method'
        required: true
        default: 'playwright'
        type: choice
        options:
          - playwright
          - apify-rag
          - apify-crawl4ai
          - firecrawl
          - all
      extract_assets:
        description: 'Download images/CSS/JS'
        required: false
        default: true
        type: boolean
      deploy_cloudflare:
        description: 'Deploy to Cloudflare Pages'
        required: false
        default: true
        type: boolean

env:
  APIFY_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
  FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: '20'

      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          npm init -y
          npm install playwright axios cheerio
          npx playwright install chromium
          pip install requests beautifulsoup4 httpx

      - name: Method 1 - Playwright (Local Headless)
        if: ${{ inputs.scrape_method == 'playwright' || inputs.scrape_method == 'all' }}
        run: |
          mkdir -p results/playwright
          node << 'SCRIPT'
          const { chromium } = require('playwright');
          const fs = require('fs');
          
          (async () => {
            console.log('ðŸŽ­ Playwright: Launching browser...');
            const browser = await chromium.launch();
            const page = await browser.newPage();
            await page.setViewportSize({ width: 1920, height: 1080 });
            
            console.log('ðŸ“„ Navigating to ${{ inputs.url }}...');
            await page.goto('${{ inputs.url }}', { 
              waitUntil: 'networkidle',
              timeout: 60000 
            });
            await page.waitForTimeout(5000);
            
            // Extract __NEXT_DATA__ for Next.js sites
            const nextData = await page.evaluate(() => {
              const script = document.getElementById('__NEXT_DATA__');
              return script ? script.textContent : null;
            });
            if (nextData) {
              fs.writeFileSync('results/playwright/next_data.json', nextData);
              console.log('âœ… Extracted __NEXT_DATA__');
            }
            
            // Get rendered HTML
            const html = await page.content();
            fs.writeFileSync('results/playwright/rendered.html', html);
            
            // Extract all inline styles
            const styles = await page.evaluate(() => {
              return Array.from(document.querySelectorAll('style'))
                .map(s => s.textContent).join('\n');
            });
            fs.writeFileSync('results/playwright/styles.css', styles);
            
            // Screenshot
            await page.screenshot({ path: 'results/playwright/screenshot.png', fullPage: true });
            
            // Extract all text content for RAG
            const textContent = await page.evaluate(() => document.body.innerText);
            fs.writeFileSync('results/playwright/content.txt', textContent);
            
            await browser.close();
            console.log('âœ… Playwright scraping complete');
          })();
          SCRIPT

      - name: Method 2 - Apify RAG Web Browser
        if: ${{ inputs.scrape_method == 'apify-rag' || inputs.scrape_method == 'all' }}
        run: |
          mkdir -p results/apify-rag
          
          # Call Apify RAG Web Browser Actor
          RESPONSE=$(curl -s -X POST \
            "https://api.apify.com/v2/acts/apify~rag-web-browser/runs?token=$APIFY_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{
              "startUrls": [{"url": "${{ inputs.url }}"}],
              "maxCrawlPages": 5,
              "outputFormats": ["markdown", "html", "text"],
              "crawlerType": "playwright:adaptive"
            }')
          
          RUN_ID=$(echo $RESPONSE | jq -r '.data.id')
          echo "Apify Run ID: $RUN_ID"
          
          # Wait for completion
          for i in {1..30}; do
            STATUS=$(curl -s "https://api.apify.com/v2/acts/apify~rag-web-browser/runs/$RUN_ID?token=$APIFY_TOKEN" | jq -r '.data.status')
            echo "Status: $STATUS"
            if [ "$STATUS" == "SUCCEEDED" ]; then break; fi
            if [ "$STATUS" == "FAILED" ]; then echo "Failed!"; exit 1; fi
            sleep 10
          done
          
          # Get results
          curl -s "https://api.apify.com/v2/acts/apify~rag-web-browser/runs/$RUN_ID/dataset/items?token=$APIFY_TOKEN" \
            -o results/apify-rag/output.json
          
          # Extract markdown content
          cat results/apify-rag/output.json | jq -r '.[0].markdown // empty' > results/apify-rag/content.md
          cat results/apify-rag/output.json | jq -r '.[0].html // empty' > results/apify-rag/content.html
          
          echo "âœ… Apify RAG scraping complete"

      - name: Method 3 - Apify Crawl4AI (LLM-Optimized)
        if: ${{ inputs.scrape_method == 'apify-crawl4ai' || inputs.scrape_method == 'all' }}
        run: |
          mkdir -p results/crawl4ai
          
          # Call Crawl4AI Actor
          RESPONSE=$(curl -s -X POST \
            "https://api.apify.com/v2/acts/raizen~ai-web-scraper/runs?token=$APIFY_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{
              "url": "${{ inputs.url }}",
              "crawlerRunOptions": {
                "exclude_external_links": true,
                "word_count_threshold": 10,
                "excluded_tags": ["form", "header", "footer", "nav"]
              },
              "scrapeOptions": {
                "formats": ["markdown", "cleaned_html", "links"]
              }
            }')
          
          RUN_ID=$(echo $RESPONSE | jq -r '.data.id')
          echo "Crawl4AI Run ID: $RUN_ID"
          
          # Wait for completion
          for i in {1..20}; do
            STATUS=$(curl -s "https://api.apify.com/v2/acts/raizen~ai-web-scraper/runs/$RUN_ID?token=$APIFY_TOKEN" | jq -r '.data.status')
            if [ "$STATUS" == "SUCCEEDED" ]; then break; fi
            if [ "$STATUS" == "FAILED" ]; then echo "Failed"; break; fi
            sleep 5
          done
          
          # Get results
          curl -s "https://api.apify.com/v2/acts/raizen~ai-web-scraper/runs/$RUN_ID/dataset/items?token=$APIFY_TOKEN" \
            -o results/crawl4ai/output.json
          
          echo "âœ… Crawl4AI scraping complete"

      - name: Method 4 - Firecrawl (Anti-Bot)
        if: ${{ inputs.scrape_method == 'firecrawl' || inputs.scrape_method == 'all' }}
        continue-on-error: true
        run: |
          mkdir -p results/firecrawl
          
          # Check if we have Firecrawl API key
          if [ -z "$FIRECRAWL_API_KEY" ]; then
            echo "âš ï¸ FIRECRAWL_API_KEY not set, skipping"
            exit 0
          fi
          
          # Call Firecrawl API
          curl -s -X POST \
            "https://api.firecrawl.dev/v1/scrape" \
            -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
            -H "Content-Type: application/json" \
            -d '{
              "url": "${{ inputs.url }}",
              "formats": ["markdown", "html", "screenshot"],
              "onlyMainContent": true,
              "waitFor": 5000
            }' -o results/firecrawl/output.json
          
          # Extract content
          cat results/firecrawl/output.json | jq -r '.data.markdown // empty' > results/firecrawl/content.md
          cat results/firecrawl/output.json | jq -r '.data.html // empty' > results/firecrawl/content.html
          
          echo "âœ… Firecrawl scraping complete"

      - name: Merge Results & Build Static Site
        run: |
          mkdir -p ${{ inputs.project_name }}/dist
          
          node << 'SCRIPT'
          const fs = require('fs');
          const path = require('path');
          
          // Find best HTML source
          const sources = [
            'results/playwright/rendered.html',
            'results/apify-rag/content.html',
            'results/crawl4ai/output.json',
            'results/firecrawl/content.html'
          ];
          
          let bestHtml = '';
          let bestSource = '';
          
          for (const src of sources) {
            try {
              if (fs.existsSync(src)) {
                let content = fs.readFileSync(src, 'utf-8');
                if (src.endsWith('.json')) {
                  const json = JSON.parse(content);
                  content = json[0]?.html || json.data?.html || '';
                }
                if (content.length > bestHtml.length) {
                  bestHtml = content;
                  bestSource = src;
                }
              }
            } catch (e) {}
          }
          
          console.log(`Best source: ${bestSource} (${bestHtml.length} chars)`);
          
          // Merge styles from Playwright if available
          let styles = '';
          try {
            styles = fs.readFileSync('results/playwright/styles.css', 'utf-8');
          } catch (e) {}
          
          // Inject styles into HTML
          if (styles && bestHtml.includes('</head>')) {
            bestHtml = bestHtml.replace('</head>', `<style>${styles}</style></head>`);
          }
          
          // Remove hydration scripts that will fail
          bestHtml = bestHtml.replace(/<script[^>]*\/_next\/static[^>]*><\/script>/g, '');
          bestHtml = bestHtml.replace(/<script[^>]*__NEXT_DATA__[^>]*>[\s\S]*?<\/script>/g, '');
          
          fs.writeFileSync('${{ inputs.project_name }}/dist/index.html', bestHtml);
          console.log('âœ… Created dist/index.html');
          
          // Copy screenshot if available
          if (fs.existsSync('results/playwright/screenshot.png')) {
            fs.copyFileSync('results/playwright/screenshot.png', '${{ inputs.project_name }}/dist/screenshot.png');
          }
          
          // Create metadata
          const meta = {
            url: '${{ inputs.url }}',
            scraped_at: new Date().toISOString(),
            method: '${{ inputs.scrape_method }}',
            sources_found: sources.filter(s => fs.existsSync(s)),
            best_source: bestSource,
            html_size: bestHtml.length
          };
          fs.writeFileSync('${{ inputs.project_name }}/dist/meta.json', JSON.stringify(meta, null, 2));
          SCRIPT

      - name: Download Assets
        if: ${{ inputs.extract_assets }}
        run: |
          cd ${{ inputs.project_name }}/dist
          
          python3 << 'PYEOF'
          import re
          import os
          import requests
          from urllib.parse import urljoin, urlparse
          
          base_url = '${{ inputs.url }}'
          
          # Read HTML
          with open('index.html', 'r') as f:
              html = f.read()
          
          # Create asset dirs
          os.makedirs('assets/images', exist_ok=True)
          os.makedirs('assets/css', exist_ok=True)
          os.makedirs('assets/js', exist_ok=True)
          
          # Download images
          img_pattern = r'<img[^>]+src=["\']([^"\']+)["\']'
          for match in re.finditer(img_pattern, html):
              src = match.group(1)
              if src.startswith('data:'):
                  continue
              try:
                  full_url = urljoin(base_url, src)
                  resp = requests.get(full_url, timeout=10)
                  if resp.ok:
                      filename = os.path.basename(urlparse(src).path) or 'image.jpg'
                      filepath = f'assets/images/{filename}'
                      with open(filepath, 'wb') as f:
                          f.write(resp.content)
                      html = html.replace(src, filepath)
                      print(f'âœ… {filename}')
              except Exception as e:
                  print(f'âš ï¸ {src}: {e}')
          
          # Save updated HTML
          with open('index.html', 'w') as f:
              f.write(html)
          
          print('âœ… Asset download complete')
          PYEOF

      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.project_name }}-scraped
          path: |
            results/
            ${{ inputs.project_name }}/
          retention-days: 30

      - name: Deploy to Cloudflare Pages
        if: ${{ inputs.deploy_cloudflare }}
        uses: cloudflare/pages-action@v1
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          projectName: ${{ inputs.project_name }}
          directory: ${{ inputs.project_name }}/dist

      - name: Summary
        run: |
          echo "## ðŸ•·ï¸ Website Scraper V2 Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**URL:** ${{ inputs.url }}" >> $GITHUB_STEP_SUMMARY
          echo "**Method:** ${{ inputs.scrape_method }}" >> $GITHUB_STEP_SUMMARY
          echo "**Project:** ${{ inputs.project_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Results" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          ls -la results/ 2>/dev/null || echo "No results dir"
          ls -la ${{ inputs.project_name }}/dist/ 2>/dev/null || echo "No dist dir"
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          if [ "${{ inputs.deploy_cloudflare }}" == "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**ðŸ”— Live URL:** https://${{ inputs.project_name }}.pages.dev" >> $GITHUB_STEP_SUMMARY
          fi
